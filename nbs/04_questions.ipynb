{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question Generation\n",
    "\n",
    "> Question Generation: Here we will put together classes / methods that provides Question Generation workflow for our passages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp question_generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bengsoon/conda/envs/xcs224/lib/python3.9/site-packages/beir/datasets/data_loader.py:2: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "from loguru import logger\n",
    "import os\n",
    "from pathlib import Path\n",
    "from fastcore.basics import patch_to, patch\n",
    "from typing import Union, List\n",
    "\n",
    "from zeroqaret.dataset import BEIRDataset, our_list as eval_list\n",
    "from zeroqaret.helper import get_today, create_header\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "\n",
    "import textwrap\n",
    "\n",
    "from colbert.modeling.colbert import ColBERT\n",
    "from colbert.infra import Run, RunConfig, ColBERTConfig\n",
    "from colbert.data import Queries, Collection\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from langchain.llms import Ollama\n",
    "from langchain.embeddings import OllamaEmbeddings\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler \n",
    "from langchain.schema import OutputParserException\n",
    "\n",
    "from typing import Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema import StrOutputParser\n",
    "from langchain.output_parsers import StructuredOutputParser, ResponseSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-10-28 19:09:37.211\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mzeroqaret.dataset\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m51\u001b[0m - \u001b[1mDatasets will be saved in '/home/bengsoon/Projects/xcs224u_project/zeroqaret/datasets'\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "beir_dataset = BEIRDataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will load the corpus for `trec-covid` as a start. We will not load the `queries` in \"reality\", we do not have access to these:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-10-28 19:09:37.238\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mzeroqaret.dataset\u001b[0m:\u001b[36mload_dataset\u001b[0m:\u001b[36m62\u001b[0m - \u001b[1mDownloading dataset 'trec-covid'...\u001b[0m\n",
      "\u001b[32m2023-10-28 19:09:37.239\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mzeroqaret.dataset\u001b[0m:\u001b[36mload_dataset\u001b[0m:\u001b[36m65\u001b[0m - \u001b[1mSaved on '/home/bengsoon/Projects/xcs224u_project/zeroqaret/datasets/trec-covid'\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27e155d6ce0c454fbb0bbdfa42b3e176",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/171332 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_name = \"trec-covid\"\n",
    "\n",
    "# We want to load only the corpus / passages  \n",
    "raw_corpus, _, _ = beir_dataset.load_dataset(dataset_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's convert `trec_corpus` into a list of passages, but first let's map the positional indices of the list to the original dataset's `pid`(s) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "trec_corpus_ids = {idx: str(val) for idx, val in enumerate(list(raw_corpus))}\n",
    "trec_corpus = [(passage.get(\"title\", \"\") + \" \" + passage[\"text\"].strip()).strip() for passage in raw_corpus.values()] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the first 5 samples of the passages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clinical features of culture-proven Mycoplasma pneumoniae infections at King Abdulaziz University Hospital, Jeddah, Saudi Arabia OBJECTIVE: This retrospective chart review describes the epidemiology and clinical features of 40 patients with culture-proven Mycoplasma pneumoniae infections at King Abdulaziz University Hospital, Jeddah, Saudi Arabia. METHODS: Patients with positive M. pneumoniae cultures from respiratory specimens from January 1997 through December 1998 were identified through the Microbiology records. Charts of patients were reviewed. RESULTS: 40 patients were identified, 33 (82.5%) of whom required admission. Most infections (92.5%) were community-acquired. The infection affected all age groups but was most common in infants (32.5%) and pre-school children (22.5%). It occurred year-round but was most common in the fall (35%) and spring (30%). More than three-quarters of patients (77.5%) had comorbidities. Twenty-four isolates (60%) were associated with pneumonia, 14 (35%) with upper respiratory tract infections, and 2 (5%) with bronchiolitis. Cough (82.5%), fever (75%), and malaise (58.8%) were the most common symptoms, and crepitations (60%), and wheezes (40%) were the most common signs. Most patients with pneumonia had crepitations (79.2%) but only 25% had bronchial breathing. Immunocompromised patients were more likely than non-immunocompromised patients to present with pneumonia (8/9 versus 16/31, P = 0.05). Of the 24 patients with pneumonia, 14 (58.3%) had uneventful recovery, 4 (16.7%) recovered following some complications, 3 (12.5%) died because of M pneumoniae infection, and 3 (12.5%) died due to underlying comorbidities. The 3 patients who died of M pneumoniae pneumonia had other comorbidities. CONCLUSION: our results were similar to published data except for the finding that infections were more common in infants and preschool children and that the mortality rate of pneumonia in patients with comorbidities was high.\n",
      "\n",
      "Nitric oxide: a pro-inflammatory mediator in lung disease? Inflammatory diseases of the respiratory tract are commonly associated with elevated production of nitric oxide (NO•) and increased indices of NO• -dependent oxidative stress. Although NO• is known to have anti-microbial, anti-inflammatory and anti-oxidant properties, various lines of evidence support the contribution of NO• to lung injury in several disease models. On the basis of biochemical evidence, it is often presumed that such NO• -dependent oxidations are due to the formation of the oxidant peroxynitrite, although alternative mechanisms involving the phagocyte-derived heme proteins myeloperoxidase and eosinophil peroxidase might be operative during conditions of inflammation. Because of the overwhelming literature on NO• generation and activities in the respiratory tract, it would be beyond the scope of this commentary to review this area comprehensively. Instead, it focuses on recent evidence and concepts of the presumed contribution of NO• to inflammatory diseases of the lung.\n",
      "\n",
      "Surfactant protein-D and pulmonary host defense Surfactant protein-D (SP-D) participates in the innate response to inhaled microorganisms and organic antigens, and contributes to immune and inflammatory regulation within the lung. SP-D is synthesized and secreted by alveolar and bronchiolar epithelial cells, but is also expressed by epithelial cells lining various exocrine ducts and the mucosa of the gastrointestinal and genitourinary tracts. SP-D, a collagenous calcium-dependent lectin (or collectin), binds to surface glycoconjugates expressed by a wide variety of microorganisms, and to oligosaccharides associated with the surface of various complex organic antigens. SP-D also specifically interacts with glycoconjugates and other molecules expressed on the surface of macrophages, neutrophils, and lymphocytes. In addition, SP-D binds to specific surfactant-associated lipids and can influence the organization of lipid mixtures containing phosphatidylinositol in vitro. Consistent with these diverse in vitro activities is the observation that SP-D-deficient transgenic mice show abnormal accumulations of surfactant lipids, and respond abnormally to challenge with respiratory viruses and bacterial lipopolysaccharides. The phenotype of macrophages isolated from the lungs of SP-D-deficient mice is altered, and there is circumstantial evidence that abnormal oxidant metabolism and/or increased metalloproteinase expression contributes to the development of emphysema. The expression of SP-D is increased in response to many forms of lung injury, and deficient accumulation of appropriately oligomerized SP-D might contribute to the pathogenesis of a variety of human lung diseases.\n",
      "\n",
      "Role of endothelin-1 in lung disease Endothelin-1 (ET-1) is a 21 amino acid peptide with diverse biological activity that has been implicated in numerous diseases. ET-1 is a potent mitogen regulator of smooth muscle tone, and inflammatory mediator that may play a key role in diseases of the airways, pulmonary circulation, and inflammatory lung diseases, both acute and chronic. This review will focus on the biology of ET-1 and its role in lung disease.\n",
      "\n",
      "Gene expression in epithelial cells in response to pneumovirus infection Respiratory syncytial virus (RSV) and pneumonia virus of mice (PVM) are viruses of the family Paramyxoviridae, subfamily pneumovirus, which cause clinically important respiratory infections in humans and rodents, respectively. The respiratory epithelial target cells respond to viral infection with specific alterations in gene expression, including production of chemoattractant cytokines, adhesion molecules, elements that are related to the apoptosis response, and others that remain incompletely understood. Here we review our current understanding of these mucosal responses and discuss several genomic approaches, including differential display reverse transcription-polymerase chain reaction (PCR) and gene array strategies, that will permit us to unravel the nature of these responses in a more complete and systematic manner.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\\n\".join(trec_corpus[:5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_model_name = \"mistral:instruct\" # for 4-bit q: use `mistral:instruct`. for 8-bit q: use `mistral:7b-instruct-q8_0`.\n",
    "base_url = \"http://localhost:11434\"\n",
    "\n",
    "\n",
    "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
    "\n",
    "llm = Ollama(base_url=base_url,\n",
    "             model=llm_model_name, \n",
    "             callback_manager = callback_manager)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if we are connected to our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm Mistral, a language model trained by the Mistral AI team."
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"I'm Mistral, a language model trained by the Mistral AI team.\""
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm(\"Can you tell me who you are?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am unable to perform actions as I do not have the ability to execute commands or interact with the physical world. I can only provide information, answer questions, and engage in text-based conversation."
     ]
    },
    {
     "data": {
      "text/plain": [
       "'I am unable to perform actions as I do not have the ability to execute commands or interact with the physical world. I can only provide information, answer questions, and engage in text-based conversation.'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm(\"What can you do?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Great! Now let's create a pipeline for question generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question Generation with `Mistral-7B-4q` & Round-trip Consistency Check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Generate Question (QG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question Generation template\n",
    "qg_prompt_template = \"\"\"<s>[INST]\n",
    "You are a curious person who loves to ask pertinent questions. Given the Passage below, it is your job to give a correct highly descriptive title, ask the relevant right question and correct one-sentenced short answer strictly from the given passage.\n",
    "----\n",
    "{format_instructions}\n",
    "---- \n",
    "Passage: {passage}\n",
    "----\n",
    "Title:\n",
    "Question: \n",
    "Answer:\n",
    "[/INST]\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create `response_schemas` so that we can instruct the model to output in a specific JSON format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create response schemas\n",
    "qg_response_schemas = [\n",
    "    # ResponseSchema(name=\"passage\", description=\"Repeat of the input passage\"), \n",
    "    ResponseSchema(name=\"title\", description=\"Descriptive generated title based on the passage\"),\n",
    "    ResponseSchema(name=\"question\", description=\"Relevant generated question based on the passage\"),\n",
    "    ResponseSchema(name=\"answer\", description=\"Generated one-sentenced short answer based on the generated question and passage\"),\n",
    "\n",
    "]\n",
    "\n",
    "# create an output parser\n",
    "qg_output_parser = StructuredOutputParser.from_response_schemas(qg_response_schemas)\n",
    "\n",
    "# get format instructions to enforce the expected json format\n",
    "qg_format_instructions = qg_output_parser.get_format_instructions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\n",
      "\n",
      "```json\n",
      "{\n",
      "\t\"title\": string  // Descriptive generated title based on the passage\n",
      "\t\"question\": string  // Relevant generated question based on the passage\n",
      "\t\"answer\": string  // Generated one-sentenced short answer based on the generated question and passage\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "print(qg_format_instructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "qg_prompt = PromptTemplate(\n",
    "    template=qg_prompt_template,\n",
    "    input_variables=[\"passage\"],\n",
    "    partial_variables={\"format_instructions\": qg_format_instructions}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST]\n",
      "You are a curious person who loves to ask pertinent questions. Given the Passage below, it is your job to give a correct highly descriptive title, ask the relevant right question and correct one-sentenced short answer strictly from the given passage.\n",
      "----\n",
      "The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\n",
      "\n",
      "```json\n",
      "{\n",
      "\t\"title\": string  // Descriptive generated title based on the passage\n",
      "\t\"question\": string  // Relevant generated question based on the passage\n",
      "\t\"answer\": string  // Generated one-sentenced short answer based on the generated question and passage\n",
      "}\n",
      "```\n",
      "---- \n",
      "Passage: Sequence specific visual detection of LAMP reactions by addition of cationic polymers BACKGROUND: Development of a practical gene point-of-care testing device (g-POCT device) requires innovative detection methods for demonstrating the results of the gene amplification reaction without the use of expensive equipment. We have studied a new method for the sequence-specific visual detection of minute amounts of nucleic acids using precipitation reaction by addition of cationic polymers to amplicons of Loop mediated isothermal Amplification (LAMP). RESULTS: Oligo DNA probes labeled with different fluorescent dyes were prepared for multiple nucleic acid templates, and the templates were amplified by the LAMP reactions under the existence of the probes. At completion of the LAMP reaction, an optimal amount of low molecular weight polyethylenimine (PEI) was added, resulting in the precipitation of the insoluble LAMP amplicon-PEI complex. The fluorescently labeled Oligo DNA probes hybridized to the LAMP product were incorporated into the precipitation, and the precipitate emitted fluorescence corresponding to the amplified nucleic acid templates. The color of emitted fluorescence can be detected easily by naked eye on a conventional UV illuminator. CONCLUSION: The presence or absence of minute amount of nucleic acid templates could be detected in a simple manner through visual assessment for the color of the LAMP amplicon-PEI complex precipitate. We conclude that this detection method may facilitate development of small and simple g-POCT device.\n",
      "----\n",
      "Title:\n",
      "Question: \n",
      "Answer:\n",
      "[/INST]\n"
     ]
    }
   ],
   "source": [
    "print(qg_prompt.format_prompt(passage=trec_corpus[45]).text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "qg_chain = qg_prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "\t\"title\": \"Development of a Gene Point-of-Care Testing Device\",\n",
      "\t\"question\": \"What is the method for the sequence-specific visual detection of minute amounts of nucleic acids in Loop mediated isothermal Amplification (LAMP) reactions?\",\n",
      "\t\"answer\": \"Oligo DNA probes labeled with different fluorescent dyes are prepared for multiple nucleic acid templates. At completion of the LAMP reaction, an optimal amount of low molecular weight polyethylenimine (PEI) is added, resulting in the precipitation of the insoluble LAMP amplicon-PEI complex. The fluorescently labeled Oligo DNA probes hybridize to the LAMP product and the precipitate emits fluorescence corresponding to the amplified nucleic acid templates. The color of emitted fluorescence can be detected easily by naked eye on a conventional UV illuminator.\"\n",
      "}\n",
      "```"
     ]
    }
   ],
   "source": [
    "qg_output_trec_45 = llm(qg_prompt.format_prompt(passage=trec_corpus[45]).text, seed=158)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_output = qg_output_parser.parse(qg_output_trec_45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Oligo DNA probes labeled with different fluorescent dyes are prepared for multiple nucleic acid templates. At completion of the LAMP reaction, an optimal amount of low molecular weight polyethylenimine (PEI) is added, resulting in the precipitation of the insoluble LAMP amplicon-PEI complex. The fluorescently labeled Oligo DNA probes hybridize to the LAMP product and the precipitate emits fluorescence corresponding to the amplified nucleic acid templates. The color of emitted fluorescence can be detected easily by naked eye on a conventional UV illuminator.'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsed_output[\"answer\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Check Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_answer_prompt_template = \"\"\"<s>[INST]\n",
    "You are an expert on the topic in the passage below. Given the Title, Passage and Question below, it is your job to provide a correct and relevant one-sentenced short answer.\n",
    "----\n",
    "{format_instructions}\n",
    "----\n",
    "Passage: {passage}\n",
    "Title: {title}\n",
    "Question: {question}\n",
    "---- \n",
    "Answer:\n",
    "[/INST]\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create response schemas\n",
    "check_answer_response_schemas = [\n",
    "    # ResponseSchema(name=\"passage\", description=\"Repeat of the input passage\"), \n",
    "    ResponseSchema(name=\"answer\", description=\"Generated one-sentenced short answer based on the title, question and passage\"),\n",
    "]\n",
    "\n",
    "# create an output parser\n",
    "check_answer_output_parser = StructuredOutputParser.from_response_schemas(check_answer_response_schemas)\n",
    "\n",
    "# get format instructions to enforce the expected json format\n",
    "check_answer_format_instructions = check_answer_output_parser.get_format_instructions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_answer_prompt = PromptTemplate(\n",
    "    template=check_answer_prompt_template,\n",
    "    input_variables=[\"passage\", \"title\", \"question\"],\n",
    "    partial_variables={\"format_instructions\": check_answer_format_instructions}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST]\n",
      "You are an expert on the topic in the passage below. Given the Title, Passage and Question below, it is your job to provide a correct and relevant one-sentenced short answer.\n",
      "----\n",
      "The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\n",
      "\n",
      "```json\n",
      "{\n",
      "\t\"answer\": string  // Generated one-sentenced short answer based on the title, question and passage\n",
      "}\n",
      "```\n",
      "----\n",
      "Passage: Sequence specific visual detection of LAMP reactions by addition of cationic polymers BACKGROUND: Development of a practical gene point-of-care testing device (g-POCT device) requires innovative detection methods for demonstrating the results of the gene amplification reaction without the use of expensive equipment. We have studied a new method for the sequence-specific visual detection of minute amounts of nucleic acids using precipitation reaction by addition of cationic polymers to amplicons of Loop mediated isothermal Amplification (LAMP). RESULTS: Oligo DNA probes labeled with different fluorescent dyes were prepared for multiple nucleic acid templates, and the templates were amplified by the LAMP reactions under the existence of the probes. At completion of the LAMP reaction, an optimal amount of low molecular weight polyethylenimine (PEI) was added, resulting in the precipitation of the insoluble LAMP amplicon-PEI complex. The fluorescently labeled Oligo DNA probes hybridized to the LAMP product were incorporated into the precipitation, and the precipitate emitted fluorescence corresponding to the amplified nucleic acid templates. The color of emitted fluorescence can be detected easily by naked eye on a conventional UV illuminator. CONCLUSION: The presence or absence of minute amount of nucleic acid templates could be detected in a simple manner through visual assessment for the color of the LAMP amplicon-PEI complex precipitate. We conclude that this detection method may facilitate development of small and simple g-POCT device.\n",
      "Title: Development of a Gene Point-of-Care Testing Device\n",
      "Question: What is the method for the sequence-specific visual detection of minute amounts of nucleic acids in Loop mediated isothermal Amplification (LAMP) reactions?\n",
      "---- \n",
      "Answer:\n",
      "[/INST]\n"
     ]
    }
   ],
   "source": [
    "print(check_answer_prompt.format_prompt(passage=trec_corpus[45], title=parsed_output[\"title\"], question=parsed_output[\"question\"]).text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_answer_chain = check_answer_prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "\t\"answer\": \"The researchers detected the presence or absence of minute amounts of nucleic acid templates by visual assessment of the color of the LAMP amplicon-PEI complex precipitate.\"\n",
      "}\n",
      "```"
     ]
    }
   ],
   "source": [
    "check_answer_output_trec_45 = llm(check_answer_prompt.format_prompt(passage=trec_corpus[45], title=parsed_output[\"title\"], question=parsed_output[\"question\"]).text, seed=158)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "    \"answer\": \"The purpose of this study is to develop a practical gene point-of-care testing device using fluorescent labeled oligo DNA probes for detecting the presence or absence of minute amounts of nucleic acid templates in a simple manner through visual assessment.\"\n",
      "}\n",
      "```"
     ]
    }
   ],
   "source": [
    "check_answer_output_trec_45 = check_answer_chain.invoke({\"passage\": trec_corpus[45], \"title\": parsed_output[\"title\"], \"question\": parsed_output[\"question\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_answer_parsed_output = check_answer_output_parser.parse(check_answer_output_trec_45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QG Answer:\n",
      "\t The researchers detected the presence or absence of minute amounts of nucleic acid templates by visual assessment for the color of the LAMP amplicon-PEI complex precipitate. \n",
      "--------------------------------------------------\n",
      " CA Answer:\n",
      "\t The researchers detected the presence or absence of minute amounts of nucleic acid templates by visual assessment of the color of the LAMP amplicon-PEI complex precipitate.\n"
     ]
    }
   ],
   "source": [
    "print(f\"QG Answer:\\n\\t {parsed_output['answer']} \\n\" + \"-\"*50 + f\"\\n CA Answer:\\n\\t {check_answer_parsed_output['answer']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Find similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [],
   "source": [
    "sbert_model = SentenceTransformer('all-mpnet-base-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [],
   "source": [
    "qg_answer_emb = sbert_model.encode(parsed_output[\"answer\"])\n",
    "ca_answer_emb = sbert_model.encode(check_answer_parsed_output[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_scores = util.cos_sim(qg_answer_emb, ca_answer_emb).to(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9988]])"
      ]
     },
     "execution_count": 405,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [],
   "source": [
    "oembed = OllamaEmbeddings(base_url=base_url, model=llm_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [],
   "source": [
    "qg_answer_oemb = oembed.embed_query(parsed_output[\"answer\"])\n",
    "ca_answer_oemb = oembed.embed_query(check_answer_parsed_output[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_scores_oemb = util.cos_sim(qg_answer_oemb, ca_answer_oemb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9963]])"
      ]
     },
     "execution_count": 409,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_scores_oemb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting It Altogether (Question Generator and Round Trip Consistency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class QuestionGenerator:\n",
    "\n",
    "    def __init__(self, \n",
    "                 ollama_base_url: str = 'http://localhost:11434',\n",
    "                 ollama_model_name: str = 'mistral:instruct',\n",
    "                 random_seed: int = 158\n",
    "                ):\n",
    "\n",
    "\n",
    "        self.ollama_base_url = ollama_base_url\n",
    "        self.ollama_model_name = ollama_model_name\n",
    "        self.random_seed = random_seed\n",
    "        \n",
    "        # callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
    "        \n",
    "        self.llm = Ollama(\n",
    "            base_url=ollama_base_url,\n",
    "            model=ollama_model_name, \n",
    "                )\n",
    "        \n",
    "        self._setup_question_generator()\n",
    "        self._setup_answer_checker()\n",
    "\n",
    "    def _setup_question_generator(self,\n",
    "                                 qg_prompt_template: str = None, # LLM Prompt template\n",
    "                                ) -> None:\n",
    "        \" Sets up all the shared variables and methods for question generator \"\n",
    "\n",
    "        \n",
    "        # Question Generation template\n",
    "        self.qg_prompt_template = qg_prompt_template or \"\"\"\n",
    "<s>[INST]\n",
    "You are a curious person who loves to ask pertinent questions. \n",
    "Given the Passage below, it is your job to give a correct highly descriptive title, ask the relevant right question and correct one-sentenced short answer strictly from the given passage.\n",
    "----\n",
    "{format_instructions}\n",
    "---- \n",
    "Passage: {passage}\n",
    "----\n",
    "Title:\n",
    "Question: \n",
    "Answer:\n",
    "[/INST]\"\"\"\n",
    "\n",
    "        # create response schemas\n",
    "        self.qg_response_schemas = [\n",
    "            # ResponseSchema(name=\"passage\", description=\"Repeat of the input passage\"), \n",
    "            ResponseSchema(name=\"title\", description=\"Descriptive generated title based on the passage\"),\n",
    "            ResponseSchema(name=\"question\", description=\"Relevant generated question based on the passage\"),\n",
    "            ResponseSchema(name=\"answer\", description=\"Generated one-sentenced short answer based on the generated question and passage\"),\n",
    "        ]\n",
    "        \n",
    "        # create an output parser\n",
    "        self.qg_output_parser = StructuredOutputParser.from_response_schemas(self.qg_response_schemas)\n",
    "        \n",
    "        # get format instructions to enforce the expected json format\n",
    "        self.qg_format_instructions = self.qg_output_parser.get_format_instructions()\n",
    "\n",
    "        # prompt template\n",
    "        self.qg_prompt = PromptTemplate(\n",
    "                                template=self.qg_prompt_template,\n",
    "                                input_variables=[\"passage\"],\n",
    "                                partial_variables={\"format_instructions\": self.qg_format_instructions}\n",
    "                            )\n",
    "\n",
    "    def _setup_answer_checker(self,\n",
    "                              ac_prompt_template: str = None, # LLM Prompt template\n",
    "                             ) -> None:\n",
    "        \" Sets up all the shared variables and methods for the Answer Checker \"\n",
    "        \n",
    "        self.answer_checker_prompt_template = \"\"\"\n",
    "<s>[INST]\n",
    "You are an expert on the topic in the passage below. Given the Title, Passage and Question below, it is your job to provide a correct and relevant one-sentenced short answer.\n",
    "----\n",
    "{format_instructions}\n",
    "----\n",
    "Passage: {passage}\n",
    "Title: {title}\n",
    "Question: {question}\n",
    "---- \n",
    "Answer:\n",
    "[/INST]\n",
    "\"\"\"\n",
    "\n",
    "        # create response schemas\n",
    "        self.answer_checker_response_schemas = [\n",
    "            ResponseSchema(name=\"answer\", description=\"Generated one-sentenced short answer based on the title, question and passage\"),\n",
    "        ]\n",
    "        \n",
    "        # create an output parser\n",
    "        self.answer_checker_output_parser = StructuredOutputParser.from_response_schemas(self.answer_checker_response_schemas)\n",
    "        \n",
    "        # get format instructions to enforce the expected json format\n",
    "        self.answer_checker_format_instructions = self.answer_checker_output_parser.get_format_instructions()\n",
    "\n",
    "        # answer checker prompt\n",
    "        self.answer_checker_prompt = PromptTemplate(\n",
    "                template=self.answer_checker_prompt_template,\n",
    "                input_variables=[\"passage\", \"title\", \"question\"],\n",
    "                partial_variables={\"format_instructions\": self.answer_checker_format_instructions}\n",
    "            )\n",
    "\n",
    "    def generate_question(self,\n",
    "                          passage: str, # passage\n",
    "                          random_seed: int = None, # if provided, it will replace random_seed\n",
    "                          verbose: bool = False, # prints prompt\n",
    "                          failure_count: int = None, # current count of failure to generate \n",
    "                          max_generation_tries: int = 10, # if failure_count == max_generation_tries, it will return the llm response params as empty strings \n",
    "                           **kwargs: \"Any\",\n",
    "                         ) -> Dict[str, str]:\n",
    "        \"\"\" \n",
    "        Prompts LLM to generate title, question and answer given `passage`. \n",
    "        Returns {\"title\": generated title, \"question\": generated question, \"answer\": generated answer }\n",
    "        \"\"\" \n",
    "\n",
    "        random_seed=random_seed or self.random_seed\n",
    "        \n",
    "        prompt = self.qg_prompt.format_prompt(passage=passage).text\n",
    "        if verbose in [\"all\"]: \n",
    "            logger.info(\"\\n\" + f\"Question Generation Prompt:\".center(100) + \"\\n\\n\" + textwrap.fill(prompt, 100))\n",
    "\n",
    "        #####  The code block below checks to see if:\n",
    "        #####      - the parser could parse the response from LLM\n",
    "        #####      - the parsed results contains all the required params \n",
    "        # Inherit failure_count input, or start from 0 if it was passed\n",
    "        if not failure_count:\n",
    "            failure_count = 0\n",
    "            \n",
    "        success = False\n",
    "        while success == False:\n",
    "            logger.info(f\"Current Question Generation failure_count: {failure_count}\")\n",
    "            try:\n",
    "                res = self.llm(prompt, seed=random_seed, **kwargs)\n",
    "                res = self.qg_output_parser.parse(res)\n",
    "                assert self._output_is_dict(res), \"Results were not parsed as dict!\"\n",
    "                assert self._check_params_in_output(res, [\"title\", \"question\", \"answer\"]), \"Results does not contain all of ('title', 'question', 'answer')!\"\n",
    "                success = True\n",
    "                \n",
    "            except OutputParserException as e:\n",
    "                logger.warning(f\"Unable to parse results. Exception msg: {e}\")\n",
    "                failure_count += 1\n",
    "                logger.info(f\"Current failure count: {failure_count}\")\n",
    "                \n",
    "            except AssertionError as e:\n",
    "                logger.warning(e)\n",
    "                failure_count += 1\n",
    "                logger.info(f\"Current failure count: {failure_count}\")\n",
    "                \n",
    "            if failure_count >= max_generation_tries:\n",
    "                logger.warning(f\"#Error: Question Generation failure_count > max_generation_tries '{max_generation_tries}'.\" + \"\\{'title': '', 'question': '', 'answer': ''\\}...\")\n",
    "                res = {\"title\": \"\", \"question\": \"\", \"answer\": \"\"}\n",
    "                break\n",
    "\n",
    "        for key, val in res.items():\n",
    "            if type(val) == list:\n",
    "                res[key] = \" \".join(val)\n",
    "                \n",
    "        if verbose in [\"all\"]:\n",
    "            logger.info(\"\\n\" + f\"Question Generation Results:\".center(100) + \"\\n\\n\" + textwrap.fill(str(res), 100))\n",
    "        return res\n",
    "\n",
    "    def generate_answer_to_check(self,\n",
    "                                 passage: str, # passage\n",
    "                                 title: str, # title\n",
    "                                 question: str, # question\n",
    "                                 random_seed: int = None, # if provided, it will replace random_seed\n",
    "                                 verbose: bool = False, # prints prompt\n",
    "                                 failure_count: int = None, # current count of failure to generate \n",
    "                                 max_generation_tries: int = 10, # if failure_count == max_generation_tries, it will return the llm response params as empty strings \n",
    "                                 **kwargs: \"Any\",\n",
    "                                  ) -> Dict[str, str]:\n",
    "        \"\"\" \n",
    "        Prompts LLM to answer given `passage`, `title`, `question`.\n",
    "        Returns {\"answer\": generated answer }\n",
    "        \"\"\" \n",
    "\n",
    "        random_seed=random_seed or self.random_seed\n",
    "        \n",
    "        prompt = self.answer_checker_prompt.format_prompt(passage=passage, title=title, question=question).text\n",
    "        if verbose in [\"all\"]: \n",
    "            logger.info(\"\\n\" + f\"Answer Checker Prompt:\".center(100) + \"\\n\\n\" + textwrap.fill(prompt, 100))\n",
    "\n",
    "        #####  The code block below checks to see if:\n",
    "        #####      - the parser could parse the response from LLM\n",
    "        #####      - the parsed results contains all the required params \n",
    "        # Inherit failure_count input, or start from 0 if it was passed\n",
    "        if not failure_count:\n",
    "            failure_count = 0\n",
    "            \n",
    "        success = False\n",
    "        while success == False:\n",
    "            logger.info(f\"Current Question Generation failure_count: {failure_count}\")\n",
    "            try:\n",
    "                res = self.llm(prompt, seed=random_seed, **kwargs)\n",
    "                res = self.answer_checker_output_parser.parse(res)\n",
    "                assert self._output_is_dict(res), \"Results were not parsed as dict!\"\n",
    "                assert self._check_params_in_output(res, [\"answer\"]), \"Results does not contain all of ('answer')!\"\n",
    "                success = True\n",
    "                \n",
    "            except OutputParserException as e:\n",
    "                logger.warning(f\"#Error: Unable to parse results. Exception msg: {e}\")\n",
    "                failure_count += 1\n",
    "                logger.info(f\"Current failure count: {failure_count}\")\n",
    "                \n",
    "            except AssertionError as e:\n",
    "                logger.warning(f\"#Error: {e}\")\n",
    "                failure_count += 1\n",
    "                logger.info(f\"Current failure count: {failure_count}\")\n",
    "                \n",
    "            if failure_count >= max_generation_tries:\n",
    "                logger.warning(f\"#Error: Answer Checker failure_count > max_generation_tries '{max_generation_tries}'.\" + \"\\{'answer': ''\\}...\")\n",
    "                res = {\"answer\": \"\"}\n",
    "                break\n",
    "\n",
    "        for key, val in res.items():\n",
    "            if type(val) == list:\n",
    "                res[key] = \" \".join(val)\n",
    "        \n",
    "        if verbose in [\"all\"]:\n",
    "            logger.info(\"\\n\" + f\"Answer Checker Results:\".center(100) + \"\\n\\n\" + textwrap.fill(str(res), 100))\n",
    "            \n",
    "        return res\n",
    "\n",
    "    def _output_is_dict(self,\n",
    "                        res: Union[str, dict], # results from llm\n",
    "                      ) -> bool:\n",
    "        \" Checks to see if output is dict. \"\n",
    "\n",
    "        return type(res) == dict\n",
    "\n",
    "    def _check_params_in_output(self,\n",
    "                                res: Union[str, dict], # results from llm\n",
    "                                params: List, # list of keys (parameters) to test\n",
    "                               ) -> bool:\n",
    "        \" Checks to see if res contains all the params as keys \"\n",
    "        \n",
    "        return all([param in res for param in params])\n",
    "\n",
    "    def round_trip_question_generation(self,\n",
    "                                       passage: str, # passage\n",
    "                                       embedding_model: str = 'all-MiniLM-L6-v2', # embedding model: any SBERT emb models or 'llm' if use emb from Ollama's llm\n",
    "                                       cos_sim_cutoff: float = 0.8, # cosine-sim cutoff score to accept [-1, 1]\n",
    "                                       random_seed: int = None, # if provided, it will replace random_seed\n",
    "                                       max_generation_tries: int = 10, # if failure_count == max_generation_tries, it will return the llm response params as empty strings \n",
    "                                       verbose: str = None, # \"all\" to report everything, \"results\" to report only results. \n",
    "                                      ) -> Union[Dict[str, str], bool]:\n",
    "        \"\"\" Prompts LLM to generate title, question and answer given `passage` and performs round-trip consistency check.\n",
    "            Returns {\"title\": generated title, \"question\": generated question, \"answer\": generated answer} if passed, None if rejected.\n",
    "            Note: We have assigned the generated_results as part of the instance variable of this class for debugging purposes when we loop through the corpus. \"\"\"\n",
    "        self.rt_random_seed = random_seed or self.random_seed\n",
    "        \n",
    "        if not hasattr(self, \"emb_model\"):\n",
    "            logger.info(\"Setting up embedding model\")\n",
    "            if embedding_model == \"llm\":\n",
    "                self.emb_model = OllamaEmbeddings(base_url=base_url, model=llm_model_name).embed_query\n",
    "            else:\n",
    "                self.emb_model = SentenceTransformer(embedding_model).encode\n",
    "            \n",
    "        logger.info(\"Generating question...\")\n",
    "        self.generated_results = self.generate_question(passage, verbose=verbose, random_seed=self.rt_random_seed, max_generation_tries=max_generation_tries)\n",
    "\n",
    "        for key, val in self.generated_results.items():\n",
    "            if type(val) == list:\n",
    "                self.generated_results[key] = \" \".join(val)\n",
    "        \n",
    "        logger.info(\"Performing round-trip consistency check...\")\n",
    "        self.checker_results = self.generate_answer_to_check(passage, self.generated_results[\"title\"], self.generated_results[\"question\"], verbose=verbose, random_seed=self.rt_random_seed, max_generation_tries=max_generation_tries)\n",
    "\n",
    "        for key, val in self.checker_results.items():\n",
    "            if type(val) == list:\n",
    "                self.checker_results[key] = \" \".join(val)\n",
    "            \n",
    "        if verbose in [\"all\", \"results\"]:\n",
    "            logger.info(\"\\n\" + \".\" * 100 + \"\\n\" + \" Generated Answer: \".center(100, \" \") + \"\\n\\n\" + textwrap.fill(f\"{self.generated_results['answer']}\", 100) + \"\\n\\n\" +\n",
    "                        \"\\n\" + \" Checker's Answer: \".center(100, \" \") + \"\\n\\n\" + textwrap.fill(f\"{self.checker_results['answer']}\", 100) + \"\\n\\n\" + \".\"*100)\n",
    "\n",
    "        logger.info(\"Performing similarity calculation ...\")\n",
    "        gen_a_emb = self.emb_model(str(self.generated_results[\"answer\"])) # force it to be string\n",
    "        checker_a_emb = self.emb_model(str(self.checker_results[\"answer\"])) # force it to be string\n",
    "\n",
    "        score = self.cos_sim(gen_a_emb, checker_a_emb)\n",
    "        if verbose in [\"all\", \"results\"]: \n",
    "            logger.info(f\"Score: {score}\")\n",
    "\n",
    "        if score < cos_sim_cutoff:\n",
    "            logger.info(f\"Rejecting generated question set as it failed the consistency check\")\n",
    "            return None\n",
    "        else:\n",
    "            logger.info(f\"Passed consistency check.\")\n",
    "            return self.generated_results        \n",
    "            \n",
    "        \n",
    "    def cos_sim(self, a: Tensor, b: Tensor):\n",
    "        \"\"\"\n",
    "        Computes the cosine similarity cos_sim(a[i], b[j]) for all i and j.\n",
    "        \n",
    "        :return: Matrix with res[i][j]  = cos_sim(a[i], b[j])\n",
    "        Adapted from https://github.com/UKPLab/sentence-transformers/blob/c5f93f70eca933c78695c5bc686ceda59651ae3b/sentence_transformers/util.py\n",
    "        \"\"\"\n",
    "        if not isinstance(a, torch.Tensor):\n",
    "            a = torch.tensor(a)\n",
    "    \n",
    "        if not isinstance(b, torch.Tensor):\n",
    "            b = torch.tensor(b)\n",
    "    \n",
    "        if len(a.shape) == 1:\n",
    "            a = a.unsqueeze(0)\n",
    "    \n",
    "        if len(b.shape) == 1:\n",
    "            b = b.unsqueeze(0)\n",
    "    \n",
    "        a_norm = torch.nn.functional.normalize(a, p=2, dim=1)\n",
    "        b_norm = torch.nn.functional.normalize(b, p=2, dim=1)\n",
    "        return torch.mm(a_norm, b_norm.transpose(0, 1)).to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_generator = QuestionGenerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch_to(QuestionGenerator)\n",
    "def generate_questions_corpus(self,\n",
    "                              dataset_name: str, # name of the dataset\n",
    "                              corpus: List,\n",
    "                              corpus_ids: Dict,\n",
    "                              df_checkpoint_path: str, # (csv format) checkpoint to save csv \n",
    "                              mode: str = \"resume\", # [\"resume\", \"replace\"] - resume if continue from checkpoint, replace if generation from scratch\n",
    "                              cos_sim_cutoff: float = 0.8, # cosine-sim cutoff score to accept [-1, 1]\n",
    "                              max_roundtrip_tries: int = 10, # maximum number of roundtrip question generation trials allowed for each document\n",
    "                              max_generation_tries: int = 10, # maximum number of generation trials allowed independently for QG and Answer-Checker for each document\n",
    "                              verbose: str = None, # \"file\" if log only to file w/o stdout (full verbosity), \"all\" if full verbosity, \"results\" if only results, \"disable\" will temporarily disable system logging.\n",
    "                             ) -> None:\n",
    "    assert verbose in [None, \"all\", \"results\", \"disable\", \"file\"], 'verbose options are only [None, \"all\", \"results\", \"disable\", \"file\"]'\n",
    "    # disable logger if asked to\n",
    "    if verbose == \"disable\": \n",
    "        logger.disable(\"__main__\")\n",
    "    else:\n",
    "        logger.enable(\"__main__\")\n",
    "\n",
    "    # make checkpoint path directory if it does not exist:\n",
    "    Path(df_checkpoint_path).parents[0].mkdir(exist_ok=True)\n",
    "    \n",
    "    if verbose == \"file\":\n",
    "        logger.configure(handlers=[dict(sink=f\"./{dataset_name}_qg_log.log\", enqueue=True,format=\"[{time}] {message}\", rotation=1e+7)]) # 100 MB\n",
    "        verbose = \"all\" # set full verbosity logging to file\n",
    "        \n",
    "    if mode == \"resume\":\n",
    "        # This is when we are resuming question generation from previous checkpoint.\n",
    "        logger.info(f\"Resuming question generation from checkpoint '{df_checkpoint_path}'\")  \n",
    "        try:\n",
    "            qg_df = pd.read_csv(df_checkpoint_path, index_col=0) \n",
    "            qg_df[\"pid\"] = qg_df[\"pid\"].astype(str) # cast pid as str even though it may be just numbers\n",
    "        except:\n",
    "            raise Exception(f\"Unable to read the checkpoint file '{df_checkpoint_path}', check the df_checkpoint_path or use `mode='replace'` to start from scratch.\")\n",
    "            \n",
    "        last_checkpoint_idx = qg_df.iloc[-1].name # index corresponding to the key in corpus_ids\n",
    "        last_checkpoint_pid = qg_df.iloc[-1].pid # pid corresponding to the val in corpus_ids\n",
    "\n",
    "        # check that corpus_ids's correspond to the checkpoint index and pid\n",
    "        assert corpus_ids[last_checkpoint_idx] == last_checkpoint_pid, \"The corpus_ids \\\n",
    "provided does not tally with the idx: pid from dataframe's saved csv checkpoint. \\\n",
    "Please check corpus_ids input.\"\n",
    "        starting_id = last_checkpoint_idx + 1 # we want to start one after our last one\n",
    "        logger.info(f\"Resuming from index {starting_id} / pid {corpus_ids[starting_id]}\")\n",
    "        \n",
    "    elif mode == \"replace\":\n",
    "        # we will start from scratch\n",
    "        if Path(df_checkpoint_path).exists():\n",
    "            raise Exception(f\"{df_checkpoint_path} already exists! Please make a back up and remove the file before starting from scratch!\")\n",
    "        \n",
    "        qg_df = pd.DataFrame(columns=[\"pid\", \"passage\", \"title\", \"question\", \"answer\"])\n",
    "        starting_id = 0   \n",
    "\n",
    "    for idx in tqdm(range(starting_id, len(corpus)), \"Question Generation Progress: \"):\n",
    "        pid = corpus_ids[idx]\n",
    "        passage = corpus[idx]\n",
    "        # logger.info(\"\\n\\n\" + f\"                     {idx} - {pid} \".center(150, \"#\") + \"\\n\\n\")\n",
    "        logger.info(create_header(f\"{idx} - {pid}\"))\n",
    "        logger.info(\"\")\n",
    "        \n",
    "        similarity_cutoff = cos_sim_cutoff\n",
    "        \n",
    "        random_seed = self.random_seed\n",
    "        success = False\n",
    "        failure_count = 0\n",
    "        while success == False:           \n",
    "            # Performs roundtrip QG. Returns {\"title\": generated title, \"question\": generated question, \"answer\": generated answer} if passed, \n",
    "            #     returns None if rejected.\n",
    "            res = self.round_trip_question_generation(passage, verbose=verbose, random_seed=random_seed, max_generation_tries=max_generation_tries, cos_sim_cutoff=similarity_cutoff) \n",
    "\n",
    "            ###############################   Code block below checks to see if ###############################\n",
    "            #    - `res` is not None\n",
    "            #    - `res` is dict \n",
    "            #    - `res` has the keys {\"title\": .., \"question\": .., \"answer\": ..}\n",
    "            try:\n",
    "                assert res, \"Rejected by roundtrip check!\"\n",
    "                assert self._output_is_dict(res), \"Results were not parsed as dict!\"\n",
    "                assert self._check_params_in_output(res, [\"title\", \"question\", \"answer\"]), \"Results does not contain all of ('title', 'question', 'answer')!\"\n",
    "                success = True \n",
    "            except AssertionError as e:\n",
    "                logger.warning(f\"#Error: {e}\")\n",
    "                logger.info(f\"Current roundtrip failure count: {failure_count}\")\n",
    "                logger.info(\"Regenerating another response...\")\n",
    "                failure_count += 1\n",
    "            except:\n",
    "                logger.warning(\"#Error: Error in roundtrip check\")\n",
    "                logger.info(f\"Current roundtrip failure count: {failure_count}\")\n",
    "                logger.info(\"Regenerating another response...\")\n",
    "                failure_count += 1\n",
    "            \n",
    "            if max_roundtrip_tries - failure_count == 1:\n",
    "                if similarity_cutoff > 0.2:\n",
    "                    similarity_cutoff = similarity_cutoff - 0.1\n",
    "                    logger.info(f\"Setting cos-sim cutoff by 0.1...\")\n",
    "                random_seed = random.randint(200, 1000)\n",
    "            elif failure_count >= max_roundtrip_tries:\n",
    "                try:\n",
    "                    assert self._output_is_dict(self.generated_results), \"Results were not parsed as dict!\"\n",
    "                    assert self._check_params_in_output(self.generated_results, [\"title\", \"question\", \"answer\"]), \"Results does not contain all of ('title', 'question', 'answer')!\"\n",
    "                    logger.warning(f\"Maximum roundtrip tries exceeded '{max_roundtrip_tries}'.\" + f\"Returning previously generated results: {self.generated_results}\")\n",
    "                    res = self.generated_results\n",
    "                    break\n",
    "                except:\n",
    "                    logger.warning(f\"Maximum roundtrip tries exceeded '{max_roundtrip_tries}'.\" + \"Returning res = \\{'title': '', 'question': '', 'answer': ''\\}...\")\n",
    "                    res = {\"title\": \"\", \"question\": \"\", \"answer\": \"\"}\n",
    "                    break\n",
    "            else:\n",
    "                random_seed = random.randint(200, 1000)\n",
    "                \n",
    "        res[\"passage\"] = passage\n",
    "        res[\"pid\"] = pid\n",
    "    \n",
    "        qg_df = pd.concat((qg_df, pd.DataFrame(res, index=[idx])))\n",
    "        logger.info(f\"Saving dataframe checkpoint as '{df_checkpoint_path}.'\")\n",
    "        qg_df.to_csv(df_checkpoint_path)\n",
    "    \n",
    "    # enable logger back\n",
    "    if verbose == \"disable\": logger.enable(\"__main__\")\n",
    "\n",
    "    logger.success(\"Question Generation is completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's generate questions for `trec-covid` dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We have done until 2822 - 8ztx62z0, but as we can see here, it is going to take a long time to finish (~160 - 240 hours to finish!)\n",
    "> Let's just skip to QG for `scifact` dataset, which has lesser amount of passages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a function that pulls all that together too:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "def generate_questions(dataset_name,\n",
    "                       mode = \"resume\",\n",
    "                       max_roundtrip_tries = 2,\n",
    "                       max_generation_tries = 5,\n",
    "                       cos_sim_cutoff = 0.6,\n",
    "                       verbose = \"file\"):\n",
    "    # initialize BeirDataset\n",
    "    beir_dataset = BEIRDataset()\n",
    "    \n",
    "    # We want to load only the corpus / passages  \n",
    "    raw_corpus, _, _ = beir_dataset.load_dataset(dataset_name)\n",
    "\n",
    "    # get corpus_ids and corpus\n",
    "    corpus_ids = {idx: str(val) for idx, val in enumerate(list(raw_corpus))}\n",
    "    corpus = [(passage.get(\"title\", \"\") + \" \" + passage[\"text\"].strip()).strip() for passage in raw_corpus.values()] \n",
    "\n",
    "    # initialize QuestionGenerator()\n",
    "    question_generator = QuestionGenerator()\n",
    "\n",
    "    question_generator.generate_questions_corpus(dataset_name = dataset_name,\n",
    "                                                 corpus = corpus, \n",
    "                                                 corpus_ids=corpus_ids, \n",
    "                                                 df_checkpoint_path=f\"../datasets/{dataset_name}/qg/{dataset_name}_qg_all.csv\", \n",
    "                                                 mode=mode, \n",
    "                                                 max_roundtrip_tries = max_roundtrip_tries,\n",
    "                                                 cos_sim_cutoff = cos_sim_cutoff, \n",
    "                                                 max_generation_tries = max_generation_tries,\n",
    "                                                 verbose=verbose)\n",
    "                       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## scifact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c3d9ba2448d4e338a79a8f950fa7b6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5183 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Question Generation Progress: 100%|█████████████████████████████████████████████████████████████████████████████| 5062/5062 [5:38:42<00:00,  4.01s/it]\n"
     ]
    }
   ],
   "source": [
    "generate_questions(dataset_name=\"scifact\", mode=\"resume\", max_roundtrip_tries=5, max_generation_tries=2, cos_sim_cutoff=0.6, verbose=\"file\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nfcorpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce057ebb2fab4d0191554ccddb8bf50a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3633 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Question Generation Progress: 100%|█████████████████████████████████████████████████████████████████████████████| 2549/2549 [2:51:39<00:00,  4.04s/it]\n"
     ]
    }
   ],
   "source": [
    "generate_questions(dataset_name=\"nfcorpus\", mode=\"resume\", max_roundtrip_tries=5, max_generation_tries=2, cos_sim_cutoff=0.6, verbose=\"file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xcs224u",
   "language": "python",
   "name": "xcs224u"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
