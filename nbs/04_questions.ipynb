{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question Generation\n",
    "\n",
    "> Question Generation: Here we will put together classes / methods that provides Question Generation workflow for our passages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp question_generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 584,
   "metadata": {},
   "outputs": [],
   "source": [
    "from loguru import logger\n",
    "import os\n",
    "from pathlib import Path\n",
    "from fastcore.basics import patch_to, patch\n",
    "from typing import Union\n",
    "\n",
    "from zeroqaret.dataset import BEIRDataset, our_list as eval_list\n",
    "from zeroqaret.helper import get_today\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "\n",
    "import textwrap\n",
    "\n",
    "from colbert.modeling.colbert import ColBERT\n",
    "from colbert.infra import Run, RunConfig, ColBERTConfig\n",
    "from colbert.data import Queries, Collection\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-10-27 12:40:57.735\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mzeroqaret.dataset\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m51\u001b[0m - \u001b[1mDatasets will be saved in '/home/bengsoon/Projects/xcs224u_project/zeroqaret/datasets'\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "beir_dataset = BEIRDataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will load the corpus for `trec-covid` as a start. We will not load the `queries` in \"reality\", we do not have access to these:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 838,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdab0bb602144e9e91276dc663f93f47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/171332 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_name = \"trec-covid\"\n",
    "\n",
    "# We want to load only the corpus / passages  \n",
    "raw_corpus, _, _ = beir_dataset.load_dataset(dataset_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's convert `trec_corpus` into a list of passages, but first let's map the positional indices of the list to the original dataset's `pid`(s) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 852,
   "metadata": {},
   "outputs": [],
   "source": [
    "trec_corpus_ids = {idx: str(val) for idx, val in enumerate(list(raw_corpus))}\n",
    "trec_corpus = [(passage.get(\"title\", \"\") + \" \" + passage[\"text\"].strip()).strip() for passage in raw_corpus.values()] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the first 5 samples of the passages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 840,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clinical features of culture-proven Mycoplasma pneumoniae infections at King Abdulaziz University Hospital, Jeddah, Saudi Arabia OBJECTIVE: This retrospective chart review describes the epidemiology and clinical features of 40 patients with culture-proven Mycoplasma pneumoniae infections at King Abdulaziz University Hospital, Jeddah, Saudi Arabia. METHODS: Patients with positive M. pneumoniae cultures from respiratory specimens from January 1997 through December 1998 were identified through the Microbiology records. Charts of patients were reviewed. RESULTS: 40 patients were identified, 33 (82.5%) of whom required admission. Most infections (92.5%) were community-acquired. The infection affected all age groups but was most common in infants (32.5%) and pre-school children (22.5%). It occurred year-round but was most common in the fall (35%) and spring (30%). More than three-quarters of patients (77.5%) had comorbidities. Twenty-four isolates (60%) were associated with pneumonia, 14 (35%) with upper respiratory tract infections, and 2 (5%) with bronchiolitis. Cough (82.5%), fever (75%), and malaise (58.8%) were the most common symptoms, and crepitations (60%), and wheezes (40%) were the most common signs. Most patients with pneumonia had crepitations (79.2%) but only 25% had bronchial breathing. Immunocompromised patients were more likely than non-immunocompromised patients to present with pneumonia (8/9 versus 16/31, P = 0.05). Of the 24 patients with pneumonia, 14 (58.3%) had uneventful recovery, 4 (16.7%) recovered following some complications, 3 (12.5%) died because of M pneumoniae infection, and 3 (12.5%) died due to underlying comorbidities. The 3 patients who died of M pneumoniae pneumonia had other comorbidities. CONCLUSION: our results were similar to published data except for the finding that infections were more common in infants and preschool children and that the mortality rate of pneumonia in patients with comorbidities was high.\n",
      "\n",
      "Nitric oxide: a pro-inflammatory mediator in lung disease? Inflammatory diseases of the respiratory tract are commonly associated with elevated production of nitric oxide (NO•) and increased indices of NO• -dependent oxidative stress. Although NO• is known to have anti-microbial, anti-inflammatory and anti-oxidant properties, various lines of evidence support the contribution of NO• to lung injury in several disease models. On the basis of biochemical evidence, it is often presumed that such NO• -dependent oxidations are due to the formation of the oxidant peroxynitrite, although alternative mechanisms involving the phagocyte-derived heme proteins myeloperoxidase and eosinophil peroxidase might be operative during conditions of inflammation. Because of the overwhelming literature on NO• generation and activities in the respiratory tract, it would be beyond the scope of this commentary to review this area comprehensively. Instead, it focuses on recent evidence and concepts of the presumed contribution of NO• to inflammatory diseases of the lung.\n",
      "\n",
      "Surfactant protein-D and pulmonary host defense Surfactant protein-D (SP-D) participates in the innate response to inhaled microorganisms and organic antigens, and contributes to immune and inflammatory regulation within the lung. SP-D is synthesized and secreted by alveolar and bronchiolar epithelial cells, but is also expressed by epithelial cells lining various exocrine ducts and the mucosa of the gastrointestinal and genitourinary tracts. SP-D, a collagenous calcium-dependent lectin (or collectin), binds to surface glycoconjugates expressed by a wide variety of microorganisms, and to oligosaccharides associated with the surface of various complex organic antigens. SP-D also specifically interacts with glycoconjugates and other molecules expressed on the surface of macrophages, neutrophils, and lymphocytes. In addition, SP-D binds to specific surfactant-associated lipids and can influence the organization of lipid mixtures containing phosphatidylinositol in vitro. Consistent with these diverse in vitro activities is the observation that SP-D-deficient transgenic mice show abnormal accumulations of surfactant lipids, and respond abnormally to challenge with respiratory viruses and bacterial lipopolysaccharides. The phenotype of macrophages isolated from the lungs of SP-D-deficient mice is altered, and there is circumstantial evidence that abnormal oxidant metabolism and/or increased metalloproteinase expression contributes to the development of emphysema. The expression of SP-D is increased in response to many forms of lung injury, and deficient accumulation of appropriately oligomerized SP-D might contribute to the pathogenesis of a variety of human lung diseases.\n",
      "\n",
      "Role of endothelin-1 in lung disease Endothelin-1 (ET-1) is a 21 amino acid peptide with diverse biological activity that has been implicated in numerous diseases. ET-1 is a potent mitogen regulator of smooth muscle tone, and inflammatory mediator that may play a key role in diseases of the airways, pulmonary circulation, and inflammatory lung diseases, both acute and chronic. This review will focus on the biology of ET-1 and its role in lung disease.\n",
      "\n",
      "Gene expression in epithelial cells in response to pneumovirus infection Respiratory syncytial virus (RSV) and pneumonia virus of mice (PVM) are viruses of the family Paramyxoviridae, subfamily pneumovirus, which cause clinically important respiratory infections in humans and rodents, respectively. The respiratory epithelial target cells respond to viral infection with specific alterations in gene expression, including production of chemoattractant cytokines, adhesion molecules, elements that are related to the apoptosis response, and others that remain incompletely understood. Here we review our current understanding of these mucosal responses and discuss several genomic approaches, including differential display reverse transcription-polymerase chain reaction (PCR) and gene array strategies, that will permit us to unravel the nature of these responses in a more complete and systematic manner.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\\n\".join(trec_corpus[:5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import Ollama\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler \n",
    "\n",
    "from typing import Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_model_name = \"mistral:instruct\" # for 4-bit q: use `mistral:instruct`. for 8-bit q: use `mistral:7b-instruct-q8_0`.\n",
    "base_url = \"http://localhost:11434\"\n",
    "\n",
    "\n",
    "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
    "\n",
    "llm = Ollama(base_url=base_url,\n",
    "             model=llm_model_name, \n",
    "             callback_manager = callback_manager)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if we are connected to our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm Mistral, a language model trained by the Mistral AI team."
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"I'm Mistral, a language model trained by the Mistral AI team.\""
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm(\"Can you tell me who you are?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am unable to perform actions as I do not have the ability to execute commands or interact with the physical world. I can only provide information, answer questions, and engage in text-based conversation."
     ]
    },
    {
     "data": {
      "text/plain": [
       "'I am unable to perform actions as I do not have the ability to execute commands or interact with the physical world. I can only provide information, answer questions, and engage in text-based conversation.'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm(\"What can you do?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Great! Now let's create a pipeline for question generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question Generation with `Mistral-7B-4q` & Round-trip Consistency Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema import StrOutputParser\n",
    "from langchain.output_parsers import StructuredOutputParser, ResponseSchema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate Question (QG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question Generation template\n",
    "qg_prompt_template = \"\"\"<s>[INST]\n",
    "You are a curious person who loves to ask pertinent questions. Given the Passage below, it is your job to give a correct highly descriptive title, ask the relevant right question and correct one-sentenced short answer strictly from the given passage.\n",
    "----\n",
    "{format_instructions}\n",
    "---- \n",
    "Passage: {passage}\n",
    "----\n",
    "Title:\n",
    "Question: \n",
    "Answer:\n",
    "[/INST]\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create `response_schemas` so that we can instruct the model to output in a specific JSON format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create response schemas\n",
    "qg_response_schemas = [\n",
    "    # ResponseSchema(name=\"passage\", description=\"Repeat of the input passage\"), \n",
    "    ResponseSchema(name=\"title\", description=\"Descriptive generated title based on the passage\"),\n",
    "    ResponseSchema(name=\"question\", description=\"Relevant generated question based on the passage\"),\n",
    "    ResponseSchema(name=\"answer\", description=\"Generated one-sentenced short answer based on the generated question and passage\"),\n",
    "\n",
    "]\n",
    "\n",
    "# create an output parser\n",
    "qg_output_parser = StructuredOutputParser.from_response_schemas(qg_response_schemas)\n",
    "\n",
    "# get format instructions to enforce the expected json format\n",
    "qg_format_instructions = qg_output_parser.get_format_instructions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\n",
      "\n",
      "```json\n",
      "{\n",
      "\t\"title\": string  // Descriptive generated title based on the passage\n",
      "\t\"question\": string  // Relevant generated question based on the passage\n",
      "\t\"answer\": string  // Generated one-sentenced short answer based on the generated question and passage\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "print(qg_format_instructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "qg_prompt = PromptTemplate(\n",
    "    template=qg_prompt_template,\n",
    "    input_variables=[\"passage\"],\n",
    "    partial_variables={\"format_instructions\": qg_format_instructions}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST]\n",
      "You are a curious person who loves to ask pertinent questions. Given the Passage below, it is your job to give a correct highly descriptive title, ask the relevant right question and correct one-sentenced short answer strictly from the given passage.\n",
      "----\n",
      "The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\n",
      "\n",
      "```json\n",
      "{\n",
      "\t\"title\": string  // Descriptive generated title based on the passage\n",
      "\t\"question\": string  // Relevant generated question based on the passage\n",
      "\t\"answer\": string  // Generated one-sentenced short answer based on the generated question and passage\n",
      "}\n",
      "```\n",
      "---- \n",
      "Passage: BACKGROUND: Development of a practical gene point-of-care testing device (g-POCT device) requires innovative detection methods for demonstrating the results of the gene amplification reaction without the use of expensive equipment. We have studied a new method for the sequence-specific visual detection of minute amounts of nucleic acids using precipitation reaction by addition of cationic polymers to amplicons of Loop mediated isothermal Amplification (LAMP). RESULTS: Oligo DNA probes labeled with different fluorescent dyes were prepared for multiple nucleic acid templates, and the templates were amplified by the LAMP reactions under the existence of the probes. At completion of the LAMP reaction, an optimal amount of low molecular weight polyethylenimine (PEI) was added, resulting in the precipitation of the insoluble LAMP amplicon-PEI complex. The fluorescently labeled Oligo DNA probes hybridized to the LAMP product were incorporated into the precipitation, and the precipitate emitted fluorescence corresponding to the amplified nucleic acid templates. The color of emitted fluorescence can be detected easily by naked eye on a conventional UV illuminator. CONCLUSION: The presence or absence of minute amount of nucleic acid templates could be detected in a simple manner through visual assessment for the color of the LAMP amplicon-PEI complex precipitate. We conclude that this detection method may facilitate development of small and simple g-POCT device.\n",
      "----\n",
      "Title:\n",
      "Question: \n",
      "Answer:\n",
      "[/INST]\n"
     ]
    }
   ],
   "source": [
    "print(qg_prompt.format_prompt(passage=trec_corpus[45]).text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [],
   "source": [
    "qg_chain = qg_prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "\t\"title\": \"Simple Visual Detection Method for Gene Amplification\",\n",
      "\t\"question\": \"How did the researchers detect the presence or absence of minute amounts of nucleic acid templates?\",\n",
      "\t\"answer\": \"The researchers detected the presence or absence of minute amounts of nucleic acid templates by visual assessment for the color of the LAMP amplicon-PEI complex precipitate.\"\n",
      "}\n",
      "```"
     ]
    }
   ],
   "source": [
    "qg_output_trec_45 = llm(qg_prompt.format_prompt(passage=trec_corpus[45]).text, seed=158)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_output = output_parser.parse(qg_output_trec_45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The researchers detected the presence or absence of minute amounts of nucleic acid templates by visual assessment for the color of the LAMP amplicon-PEI complex precipitate.'"
      ]
     },
     "execution_count": 371,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsed_output[\"answer\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_answer_prompt_template = \"\"\"<s>[INST]\n",
    "You are an expert on the topic in the passage below. Given the Title, Passage and Question below, it is your job to provide a correct and relevant one-sentenced short answer.\n",
    "----\n",
    "{format_instructions}\n",
    "----\n",
    "Passage: {passage}\n",
    "Title: {title}\n",
    "Question: {question}\n",
    "---- \n",
    "Answer:\n",
    "[/INST]\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create response schemas\n",
    "check_answer_response_schemas = [\n",
    "    # ResponseSchema(name=\"passage\", description=\"Repeat of the input passage\"), \n",
    "    ResponseSchema(name=\"answer\", description=\"Generated one-sentenced short answer based on the title, question and passage\"),\n",
    "]\n",
    "\n",
    "# create an output parser\n",
    "check_answer_output_parser = StructuredOutputParser.from_response_schemas(check_answer_response_schemas)\n",
    "\n",
    "# get format instructions to enforce the expected json format\n",
    "check_answer_format_instructions = check_answer_output_parser.get_format_instructions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_answer_prompt = PromptTemplate(\n",
    "    template=check_answer_prompt_template,\n",
    "    input_variables=[\"passage\", \"title\", \"question\"],\n",
    "    partial_variables={\"format_instructions\": check_answer_format_instructions}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST]\n",
      "You are an expert on the topic in the passage below. Given the Title, Passage and Question below, it is your job to provide a correct and relevant one-sentenced short answer.\n",
      "----\n",
      "The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\n",
      "\n",
      "```json\n",
      "{\n",
      "\t\"answer\": string  // Generated one-sentenced short answer based on the title, question and passage\n",
      "}\n",
      "```\n",
      "----\n",
      "Passage: BACKGROUND: Development of a practical gene point-of-care testing device (g-POCT device) requires innovative detection methods for demonstrating the results of the gene amplification reaction without the use of expensive equipment. We have studied a new method for the sequence-specific visual detection of minute amounts of nucleic acids using precipitation reaction by addition of cationic polymers to amplicons of Loop mediated isothermal Amplification (LAMP). RESULTS: Oligo DNA probes labeled with different fluorescent dyes were prepared for multiple nucleic acid templates, and the templates were amplified by the LAMP reactions under the existence of the probes. At completion of the LAMP reaction, an optimal amount of low molecular weight polyethylenimine (PEI) was added, resulting in the precipitation of the insoluble LAMP amplicon-PEI complex. The fluorescently labeled Oligo DNA probes hybridized to the LAMP product were incorporated into the precipitation, and the precipitate emitted fluorescence corresponding to the amplified nucleic acid templates. The color of emitted fluorescence can be detected easily by naked eye on a conventional UV illuminator. CONCLUSION: The presence or absence of minute amount of nucleic acid templates could be detected in a simple manner through visual assessment for the color of the LAMP amplicon-PEI complex precipitate. We conclude that this detection method may facilitate development of small and simple g-POCT device.\n",
      "Title: Simple Visual Detection Method for Gene Amplification\n",
      "Question: How did the researchers detect the presence or absence of minute amounts of nucleic acid templates?\n",
      "---- \n",
      "Answer:\n",
      "[/INST]\n"
     ]
    }
   ],
   "source": [
    "print(check_answer_prompt.format_prompt(passage=trec_corpus[45], title=parsed_output[\"title\"], question=parsed_output[\"question\"]).text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_answer_chain = check_answer_prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "\t\"answer\": \"The researchers detected the presence or absence of minute amounts of nucleic acid templates by visual assessment of the color of the LAMP amplicon-PEI complex precipitate.\"\n",
      "}\n",
      "```"
     ]
    }
   ],
   "source": [
    "check_answer_output_trec_45 = llm(check_answer_prompt.format_prompt(passage=trec_corpus[45], title=parsed_output[\"title\"], question=parsed_output[\"question\"]).text, seed=158)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "    \"answer\": \"The purpose of this study is to develop a practical gene point-of-care testing device using fluorescent labeled oligo DNA probes for detecting the presence or absence of minute amounts of nucleic acid templates in a simple manner through visual assessment.\"\n",
      "}\n",
      "```"
     ]
    }
   ],
   "source": [
    "check_answer_output_trec_45 = check_answer_chain.invoke({\"passage\": trec_corpus[45], \"title\": parsed_output[\"title\"], \"question\": parsed_output[\"question\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_answer_parsed_output = check_answer_output_parser.parse(check_answer_output_trec_45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QG Answer:\n",
      "\t The researchers detected the presence or absence of minute amounts of nucleic acid templates by visual assessment for the color of the LAMP amplicon-PEI complex precipitate. \n",
      "--------------------------------------------------\n",
      " CA Answer:\n",
      "\t The researchers detected the presence or absence of minute amounts of nucleic acid templates by visual assessment of the color of the LAMP amplicon-PEI complex precipitate.\n"
     ]
    }
   ],
   "source": [
    "print(f\"QG Answer:\\n\\t {parsed_output['answer']} \\n\" + \"-\"*50 + f\"\\n CA Answer:\\n\\t {check_answer_parsed_output['answer']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [],
   "source": [
    "sbert_model = SentenceTransformer('all-mpnet-base-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [],
   "source": [
    "qg_answer_emb = sbert_model.encode(parsed_output[\"answer\"])\n",
    "ca_answer_emb = sbert_model.encode(check_answer_parsed_output[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_scores = util.cos_sim(qg_answer_emb, ca_answer_emb).to(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9988]])"
      ]
     },
     "execution_count": 405,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [],
   "source": [
    "oembed = OllamaEmbeddings(base_url=base_url, model=llm_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [],
   "source": [
    "qg_answer_oemb = oembed.embed_query(parsed_output[\"answer\"])\n",
    "ca_answer_oemb = oembed.embed_query(check_answer_parsed_output[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_scores_oemb = util.cos_sim(qg_answer_oemb, ca_answer_oemb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9963]])"
      ]
     },
     "execution_count": 409,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_scores_oemb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting It Altogether (Question Generator and Round Trip Consistency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mistral:instruct'"
      ]
     },
     "execution_count": 413,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcos_sim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mSource:\u001b[0m   \n",
       "\u001b[0;32mdef\u001b[0m \u001b[0mcos_sim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m\"\"\"\u001b[0m\n",
       "\u001b[0;34m    Computes the cosine similarity cos_sim(a[i], b[j]) for all i and j.\u001b[0m\n",
       "\u001b[0;34m    :return: Matrix with res[i][j]  = cos_sim(a[i], b[j])\u001b[0m\n",
       "\u001b[0;34m    \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0ma_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mb_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_norm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mFile:\u001b[0m      ~/conda/envs/xcs224/lib/python3.9/site-packages/sentence_transformers/util.py\n",
       "\u001b[0;31mType:\u001b[0m      function"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "util.cos_sim??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 853,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuestionGenerator:\n",
    "\n",
    "    def __init__(self, \n",
    "                 ollama_base_url: str = 'http://localhost:11434',\n",
    "                 ollama_model_name: str = 'mistral:instruct',\n",
    "                 random_seed: int = 158\n",
    "                ):\n",
    "\n",
    "\n",
    "        self.ollama_base_url = ollama_base_url\n",
    "        self.ollama_model_name = ollama_model_name\n",
    "        self.random_seed = random_seed\n",
    "        \n",
    "        # callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
    "        \n",
    "        self.llm = Ollama(\n",
    "            base_url=ollama_base_url,\n",
    "            model=ollama_model_name, \n",
    "                )\n",
    "        \n",
    "        self._setup_question_generator()\n",
    "        self._setup_answer_checker()\n",
    "\n",
    "    def _setup_question_generator(self,\n",
    "                                 qg_prompt_template: str = None, # LLM Prompt template\n",
    "                                ) -> None:\n",
    "        \" Sets up all the shared variables and methods for question generator \"\n",
    "\n",
    "        \n",
    "        # Question Generation template\n",
    "        self.qg_prompt_template = qg_prompt_template or \"\"\"<s>[INST]\n",
    "            You are a curious person who loves to ask pertinent questions. \n",
    "            Given the Passage below, it is your job to give a correct highly descriptive title, ask the relevant right question and correct one-sentenced short answer strictly from the given passage.\n",
    "            ----\n",
    "            {format_instructions}\n",
    "            ---- \n",
    "            Passage: {passage}\n",
    "            ----\n",
    "            Title:\n",
    "            Question: \n",
    "            Answer:\n",
    "            [/INST]\"\"\"\n",
    "\n",
    "        # create response schemas\n",
    "        self.qg_response_schemas = [\n",
    "            # ResponseSchema(name=\"passage\", description=\"Repeat of the input passage\"), \n",
    "            ResponseSchema(name=\"title\", description=\"Descriptive generated title based on the passage\"),\n",
    "            ResponseSchema(name=\"question\", description=\"Relevant generated question based on the passage\"),\n",
    "            ResponseSchema(name=\"answer\", description=\"Generated one-sentenced short answer based on the generated question and passage\"),\n",
    "        ]\n",
    "        \n",
    "        # create an output parser\n",
    "        self.qg_output_parser = StructuredOutputParser.from_response_schemas(self.qg_response_schemas)\n",
    "        \n",
    "        # get format instructions to enforce the expected json format\n",
    "        self.qg_format_instructions = self.qg_output_parser.get_format_instructions()\n",
    "\n",
    "        # prompt template\n",
    "        self.qg_prompt = PromptTemplate(\n",
    "                                template=self.qg_prompt_template,\n",
    "                                input_variables=[\"passage\"],\n",
    "                                partial_variables={\"format_instructions\": self.qg_format_instructions}\n",
    "                            )\n",
    "\n",
    "    def _setup_answer_checker(self,\n",
    "                              ac_prompt_template: str = None, # LLM Prompt template\n",
    "                             ) -> None:\n",
    "        \" Sets up all the shared variables and methods for the Answer Checker \"\n",
    "        \n",
    "        self.ac_prompt_template = \"\"\"<s>[INST]\n",
    "            You are an expert on the topic in the passage below. Given the Title, Passage and Question below, it is your job to provide a correct and relevant one-sentenced short answer.\n",
    "            ----\n",
    "            {format_instructions}\n",
    "            ----\n",
    "            Passage: {passage}\n",
    "            Title: {title}\n",
    "            Question: {question}\n",
    "            ---- \n",
    "            Answer:\n",
    "            [/INST]\"\"\"\n",
    "\n",
    "        # create response schemas\n",
    "        self.answer_checker_response_schemas = [\n",
    "            ResponseSchema(name=\"answer\", description=\"Generated one-sentenced short answer based on the title, question and passage\"),\n",
    "        ]\n",
    "        \n",
    "        # create an output parser\n",
    "        self.answer_checker_output_parser = StructuredOutputParser.from_response_schemas(self.answer_checker_response_schemas)\n",
    "        \n",
    "        # get format instructions to enforce the expected json format\n",
    "        self.answer_checker_format_instructions = self.answer_checker_output_parser.get_format_instructions()\n",
    "\n",
    "        # answer checker prompt\n",
    "        self.answer_checker_prompt = PromptTemplate(\n",
    "                template=check_answer_prompt_template,\n",
    "                input_variables=[\"passage\", \"title\", \"question\"],\n",
    "                partial_variables={\"format_instructions\": check_answer_format_instructions}\n",
    "            )\n",
    "\n",
    "    def generate_question(self,\n",
    "                          passage: str, # passage\n",
    "                          random_seed: int = None, # if provided, it will replace random_seed\n",
    "                          verbose: bool = False, # prints prompt\n",
    "                           **kwargs: \"Any\",\n",
    "                         ) -> Dict[str, str]:\n",
    "        \"\"\" Prompts LLM to generate title, question and answer given `passage` \"\"\" \n",
    "        random_seed=random_seed or self.random_seed\n",
    "        \n",
    "        prompt = self.qg_prompt.format_prompt(passage=passage).text\n",
    "        if verbose in [\"all\"]: \n",
    "            print(prompt)\n",
    "        res = self.llm(prompt, seed=random_seed, **kwargs)\n",
    "        \n",
    "        try:\n",
    "            res = self.qg_output_parser.parse(res)\n",
    "        except:\n",
    "            temp_random_seed = random.randint(200, 1000)\n",
    "            logger.info(f\"Unable to parse results. Regenerating with `random_seed = {temp_random_seed}`...\")\n",
    "            return self.generate_question(passage=passage, random_seed=temp_random_seed, verbose=verbose,  repeat_last_n=0)\n",
    "        \n",
    "        if not self._output_is_dict(res): \n",
    "            temp_random_seed = random.randint(200, 1000)\n",
    "            logger.info(f\"Generated question output is not dict. Regenerating with `random_seed = {temp_random_seed}`...\")\n",
    "            return self.generate_question(passage=passage, random_seed=temp_random_seed, verbose=verbose,  repeat_last_n=0)\n",
    "            \n",
    "        if not self._check_title_question_answer_in_dict(res):\n",
    "            temp_random_seed = random.randint(200, 1000)\n",
    "            logger.info(f\"Either all or some of ('title', 'question', 'answer') not in dict. Regenerating with `random_seed = {temp_random_seed}`...\")\n",
    "            return self.generate_question(passage=passage, random_seed=temp_random_seed, verbose=verbose,  repeat_last_n=0)\n",
    "        \n",
    "        if verbose in [\"all\"]:\n",
    "            logger.info(f\"{res}\")\n",
    "            \n",
    "        return res\n",
    "            \n",
    "  \n",
    "            \n",
    "\n",
    "    def generate_answer_to_check(self,\n",
    "                                 passage: str, # passage\n",
    "                                 title: str, # title\n",
    "                                 question: str, # question\n",
    "                                 random_seed: int = None, # if provided, it will replace random_seed\n",
    "                                 verbose: bool = False, # prints prompt\n",
    "                                 **kwargs: \"Any\",\n",
    "                                  ) -> Dict[str, str]:\n",
    "        \"\"\" Prompts LLM to answer given `passage`, `title`, `question` \"\"\" \n",
    "        random_seed=random_seed or self.random_seed\n",
    "        \n",
    "        prompt = self.answer_checker_prompt.format_prompt(passage=passage, title=title, question=question).text\n",
    "        if verbose in [\"all\"]: \n",
    "            print(prompt)\n",
    "            \n",
    "        res = self.llm(prompt, seed=random_seed, **kwargs)\n",
    "        \n",
    "        try:\n",
    "            res = self.answer_checker_output_parser.parse(res)\n",
    "        except:\n",
    "            temp_random_seed = random.randint(200, 1000)\n",
    "            logger.info(f\"Unable to parse results. Regenerating with `random_seed = {temp_random_seed}`...\")\n",
    "            return self.generate_answer_to_check(passage=passage, title=title, question=question, verbose=verbose, random_seed=temp_random_seed, repeat_last_n=0)\n",
    "\n",
    "        if not self._output_is_dict(res): \n",
    "            temp_random_seed = random.randint(200, 1000)\n",
    "            logger.info(f\"Generated checker's answer output is not dict. Regenerating with `random_seed = {temp_random_seed}`...\")\n",
    "            return self.generate_answer_to_check(passage=passage, title=title, question=question, verbose=verbose, random_seed=temp_random_seed, repeat_last_n=0)\n",
    "\n",
    "        if not self._check_answer_in_dict(res):\n",
    "            temp_random_seed = random.randint(200, 1000)\n",
    "            logger.info(f\"'answer' not in dict. Regenerating with `random_seed = {temp_random_seed}`...\")\n",
    "            return self.generate_answer_to_check(passage=passage, title=title, question=question, verbose=verbose, random_seed=temp_random_seed, repeat_last_n=0)\n",
    "            \n",
    "        if verbose in [\"all\"]:\n",
    "            logger.info(f\"{res}\")\n",
    "            \n",
    "        return res\n",
    "\n",
    "    def _output_is_dict(self,\n",
    "                       results: Union[str, dict], # results from llm\n",
    "                      ) -> bool:\n",
    "        \" Checks to see if output is dict. \"\n",
    "\n",
    "        return type(results) == dict\n",
    "\n",
    "    def _check_title_question_answer_in_dict(self,\n",
    "                             result: dict, # results from llm\n",
    "                            ) -> bool:\n",
    "        \"\"\" \n",
    "        Check to see if all of 'title', 'question' and 'answer' are in `results`. \n",
    "        \"\"\"\n",
    "        \n",
    "\n",
    "        return (\"title\" in result) and (\"question\" in result) and (\"answer\" in result)\n",
    "\n",
    "    def _check_question_in_dict(self,\n",
    "                               result: dict, # results from llm\n",
    "                                ) -> bool:\n",
    "        \"\"\" \n",
    "        Check to see if 'question' is in `results`. \n",
    "        \"\"\"\n",
    "\n",
    "        return \"question\" in result\n",
    "        \n",
    "    def _check_answer_in_dict(self,\n",
    "                               result: dict, # results from llm\n",
    "                                ) -> bool:\n",
    "        \"\"\" \n",
    "        Check to see if 'answer' is in `results`. \n",
    "        \"\"\"\n",
    "\n",
    "        return \"answer\" in result\n",
    "        \n",
    "    def round_trip_question_generation(self,\n",
    "                                       passage: str, # passage\n",
    "                                       embedding_model: str = 'all-MiniLM-L6-v2', # embedding model: any SBERT emb models or 'llm' if use emb from Ollama's llm\n",
    "                                       cutoff: float = 0.8, # cosine-sim cutoff score to accept [-1, 1]\n",
    "                                       random_seed: int = None, # if provided, it will replace random_seed\n",
    "                                       verbose: str = None, # \"all\" to report everything, \"results\" to report only results. \n",
    "                                      ) -> Union[Dict[str, str], bool]:\n",
    "        \"\"\" Prompts LLM to generate title, question and answer given `passage` and performs round-trip consistency check. \"\"\"\n",
    "        \"\"\" Note: We have assigned the generated_results as part of the instance variable of this class for debugging purposes when we loop through the corpus. \"\"\"\n",
    "        self.rt_random_seed = random_seed or self.random_seed\n",
    "\n",
    "        if not hasattr(self, \"emb_model\"):\n",
    "            logger.info(\"Setting up embedding model\")\n",
    "            if embedding_model == \"llm\":\n",
    "                self.emb_model = OllamaEmbeddings(base_url=base_url, model=llm_model_name).embed_query\n",
    "            else:\n",
    "                self.emb_model = SentenceTransformer(embedding_model).encode\n",
    "            \n",
    "        logger.info(\"Generating question...\")\n",
    "        self.generated_results = self.generate_question(passage, verbose=verbose, random_seed=self.rt_random_seed)\n",
    "\n",
    "        logger.info(\"Performing round-trip consistency check...\")\n",
    "        self.checker_results = self.generate_answer_to_check(passage, self.generated_results[\"title\"], self.generated_results[\"question\"], verbose=verbose, random_seed=self.rt_random_seed)\n",
    "\n",
    "        if verbose in [\"all\", \"results\"]:\n",
    "            logger.info(\"\\n\" + \".\" * 100 + \"\\n\" + \" Generated Answer: \".center(100, \" \") + \"\\n\\n\" + textwrap.fill(f\"{self.generated_results['answer']}\", 100) + \"\\n\\n\" +\n",
    "                        \"\\n\" + \" Checker's Answer: \".center(100, \" \") + \"\\n\\n\" + textwrap.fill(f\"{self.checker_results['answer']}\", 100) + \"\\n\\n\" + \".\"*100)\n",
    "\n",
    "        logger.info(\"Performing similarity calculation ...\")\n",
    "        gen_a_emb = self.emb_model(self.generated_results[\"answer\"])\n",
    "        checker_a_emb = self.emb_model(self.checker_results[\"answer\"])\n",
    "\n",
    "        score = self.cos_sim(gen_a_emb, checker_a_emb)\n",
    "        if verbose in [\"all\", \"results\"]: \n",
    "            logger.info(f\"Score: {score}\")\n",
    "\n",
    "        if score < cutoff:\n",
    "            logger.info(f\"Rejecting generated question set as it failed the consistency check\")\n",
    "            return None\n",
    "        else:\n",
    "            logger.info(f\"Passed consistency check.\")\n",
    "            return self.generated_results        \n",
    "            \n",
    "        \n",
    "    def cos_sim(self, a: Tensor, b: Tensor):\n",
    "        \"\"\"\n",
    "        Computes the cosine similarity cos_sim(a[i], b[j]) for all i and j.\n",
    "        \n",
    "        :return: Matrix with res[i][j]  = cos_sim(a[i], b[j])\n",
    "        Adapted from https://github.com/UKPLab/sentence-transformers/blob/c5f93f70eca933c78695c5bc686ceda59651ae3b/sentence_transformers/util.py\n",
    "        \"\"\"\n",
    "        if not isinstance(a, torch.Tensor):\n",
    "            a = torch.tensor(a)\n",
    "    \n",
    "        if not isinstance(b, torch.Tensor):\n",
    "            b = torch.tensor(b)\n",
    "    \n",
    "        if len(a.shape) == 1:\n",
    "            a = a.unsqueeze(0)\n",
    "    \n",
    "        if len(b.shape) == 1:\n",
    "            b = b.unsqueeze(0)\n",
    "    \n",
    "        a_norm = torch.nn.functional.normalize(a, p=2, dim=1)\n",
    "        b_norm = torch.nn.functional.normalize(b, p=2, dim=1)\n",
    "        return torch.mm(a_norm, b_norm.transpose(0, 1)).to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 854,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_generator = QuestionGenerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 855,
   "metadata": {},
   "outputs": [],
   "source": [
    "@patch_to(QuestionGenerator)\n",
    "def generate_questions_corpus(self,\n",
    "                              corpus: List,\n",
    "                              corpus_ids: Dict,\n",
    "                              df_checkpoint_path: str,\n",
    "                              verbose: str = None\n",
    "                             ) -> None:\n",
    "    assert verbose in [None, \"all\", \"results\", \"disable\"], 'verbose options are only [None, \"all\", \"results\", \"disable\"]'\n",
    "    \n",
    "    # disable logger if asked to\n",
    "    if verbose == \"disable\": \n",
    "        logger.disable(\"__main__\")\n",
    "    else:\n",
    "        logger.enable(\"__main__\")\n",
    "    \n",
    "    # logger.add(f\"./{get_today('%Y%m%d')}_qg.log\")\n",
    "\n",
    "    qg_df = pd.DataFrame(columns=[\"pid\", \"passage\", \"title\", \"question\", \"answer\"])\n",
    "\n",
    "    for idx, passage in enumerate(tqdm(corpus, \"Question Generation Progress: \", len(corpus))):\n",
    "        pid = corpus_ids[idx]\n",
    "        logger.info(\"\\n\" + f\" {idx+1} - {pid} \".center(150, \"#\"))\n",
    "        logger.info(\"\")\n",
    "        res = self.round_trip_question_generation(passage, verbose=verbose)\n",
    "    \n",
    "        while not res:\n",
    "            temp_rand_seed = random.randint(200, 1000)\n",
    "            logger.info(f\"Retrying again with a different seed (`random_seed = {temp_rand_seed}`) ...\")\n",
    "            res = self.round_trip_question_generation(passage, verbose=verbose, random_seed=temp_rand_seed)\n",
    "            \n",
    "        res[\"passage\"] = passage\n",
    "        res[\"pid\"] = pid\n",
    "    \n",
    "        qg_df = pd.concat((qg_df, pd.DataFrame(res, index=[idx])))\n",
    "        \n",
    "        if idx % 5 == 0:\n",
    "            logger.info(f\"Saving dataframe checkpoint as '{df_checkpoint_path}'\")\n",
    "            qg_df.to_csv(df_checkpoint_path)\n",
    "        logger.info(\"\")\n",
    "\n",
    "    # enable logger back\n",
    "    if verbose == \"disable\": logger.enable(\"__main__\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's generate questions for `trec-covid` dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Question Generation Progress:   0%| | 0/1 [00:00<?\u001b[32m2023-10-28 02:25:38.096\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgenerate_questions_corpus\u001b[0m:\u001b[36m22\u001b[0m - \u001b[1m\n",
      "#################################################################### 1 - 8qnrcgnk ####################################################################\u001b[0m\n",
      "\u001b[32m2023-10-28 02:25:38.097\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgenerate_questions_corpus\u001b[0m:\u001b[36m23\u001b[0m - \u001b[1m\u001b[0m\n",
      "\u001b[32m2023-10-28 02:25:38.099\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mround_trip_question_generation\u001b[0m:\u001b[36m225\u001b[0m - \u001b[1mSetting up embedding model\u001b[0m\n",
      "\u001b[32m2023-10-28 02:25:38.310\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mround_trip_question_generation\u001b[0m:\u001b[36m231\u001b[0m - \u001b[1mGenerating question...\u001b[0m\n",
      "\u001b[32m2023-10-28 02:25:40.138\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mround_trip_question_generation\u001b[0m:\u001b[36m234\u001b[0m - \u001b[1mPerforming round-trip consistency check...\u001b[0m\n",
      "\u001b[32m2023-10-28 02:25:42.847\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mround_trip_question_generation\u001b[0m:\u001b[36m241\u001b[0m - \u001b[1mPerforming similarity calculation ...\u001b[0m\n",
      "\u001b[32m2023-10-28 02:25:42.896\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mround_trip_question_generation\u001b[0m:\u001b[36m250\u001b[0m - \u001b[1mRejecting generated question set as it failed the consistency check\u001b[0m\n",
      "\u001b[32m2023-10-28 02:25:42.898\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgenerate_questions_corpus\u001b[0m:\u001b[36m28\u001b[0m - \u001b[1mRetrying again with a different seed (`random_seed = 209`) ...\u001b[0m\n",
      "\u001b[32m2023-10-28 02:25:42.900\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mround_trip_question_generation\u001b[0m:\u001b[36m231\u001b[0m - \u001b[1mGenerating question...\u001b[0m\n",
      "\u001b[32m2023-10-28 02:25:45.513\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mround_trip_question_generation\u001b[0m:\u001b[36m234\u001b[0m - \u001b[1mPerforming round-trip consistency check...\u001b[0m\n",
      "\u001b[32m2023-10-28 02:25:47.744\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mround_trip_question_generation\u001b[0m:\u001b[36m241\u001b[0m - \u001b[1mPerforming similarity calculation ...\u001b[0m\n",
      "\u001b[32m2023-10-28 02:25:47.765\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mround_trip_question_generation\u001b[0m:\u001b[36m253\u001b[0m - \u001b[1mPassed consistency check.\u001b[0m\n",
      "\u001b[32m2023-10-28 02:25:47.768\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgenerate_questions_corpus\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1mSaving dataframe checkpoint as '../datasets/trec-covid/qg/trec-covid_qg_test.csv'\u001b[0m\n",
      "\u001b[32m2023-10-28 02:25:47.771\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgenerate_questions_corpus\u001b[0m:\u001b[36m39\u001b[0m - \u001b[1m\u001b[0m\n",
      "Question Generation Progress: 100%|█| 1/1 [00:09<0\n",
      "Question Generation Progress:   0%| | 0/1 [00:00<?\u001b[32m2023-10-28 02:25:47.776\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgenerate_questions_corpus\u001b[0m:\u001b[36m22\u001b[0m - \u001b[1m\n",
      "#################################################################### 1 - 8qnrcgnk ####################################################################\u001b[0m\n",
      "\u001b[32m2023-10-28 02:25:47.777\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgenerate_questions_corpus\u001b[0m:\u001b[36m23\u001b[0m - \u001b[1m\u001b[0m\n",
      "\u001b[32m2023-10-28 02:25:47.778\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mround_trip_question_generation\u001b[0m:\u001b[36m231\u001b[0m - \u001b[1mGenerating question...\u001b[0m\n",
      "\u001b[32m2023-10-28 02:25:50.086\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mround_trip_question_generation\u001b[0m:\u001b[36m234\u001b[0m - \u001b[1mPerforming round-trip consistency check...\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "test_corpus = trec_corpus[8:9]\n",
    "test_corpus_ids = {idx: trec_corpus_ids[i] for idx, i in enumerate(range(8,9))}\n",
    "\n",
    "for i in range(10):\n",
    "    question_generator.generate_questions_corpus(test_corpus, test_corpus_ids, f\"../datasets/{dataset_name}/qg/{dataset_name}_qg_test.csv\")\n",
    "\n",
    "logger.info(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xcs224u",
   "language": "python",
   "name": "xcs224u"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
