{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "\n",
    "> Evaluation: Evaluators for zeroqaret project - ColBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bengsoon/conda/envs/xcs224/lib/python3.9/site-packages/beir/datasets/data_loader.py:2: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "from loguru import logger\n",
    "import os\n",
    "from pathlib import Path\n",
    "from fastcore.basics import patch_to, patch\n",
    "\n",
    "from zeroqaret.helper import *\n",
    "from zeroqaret.dataset import BEIRDataset, our_list as eval_list\n",
    "\n",
    "from getpass import getpass\n",
    "from typing import Union, Dict, List\n",
    "\n",
    "from colbert import Indexer, Searcher\n",
    "from colbert.infra import Run, RunConfig, ColBERTConfig\n",
    "from colbert.data import Queries, Collection\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # \n",
    "# mlflow_uri = getpass(\"Enter mlflow uri: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# setup_mlflow(mlflow_uri, \"zeroqaret\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Get Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fiqa', 'trec-covid']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# these are the list of datasets to be evaluated\n",
    "eval_list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-10-27 00:37:15.805\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mzeroqaret.dataset\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m51\u001b[0m - \u001b[1mDatasets will be saved in '/home/bengsoon/Projects/xcs224u_project/zeroqaret/datasets'\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "beir_datasets = BEIRDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-10-27 00:37:15.822\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mzeroqaret.dataset\u001b[0m:\u001b[36mload_dataset\u001b[0m:\u001b[36m62\u001b[0m - \u001b[1mDownloading dataset 'fiqa'...\u001b[0m\n",
      "\u001b[32m2023-10-27 00:37:15.823\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mzeroqaret.dataset\u001b[0m:\u001b[36mload_dataset\u001b[0m:\u001b[36m65\u001b[0m - \u001b[1mSaved on '/home/bengsoon/Projects/xcs224u_project/zeroqaret/datasets/fiqa'\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c8f1bc73b9f4d01bbbf2697d8bcb234",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/57638 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-10-27 00:37:16.215\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mzeroqaret.dataset\u001b[0m:\u001b[36mload_dataset\u001b[0m:\u001b[36m62\u001b[0m - \u001b[1mDownloading dataset 'trec-covid'...\u001b[0m\n",
      "\u001b[32m2023-10-27 00:37:16.216\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mzeroqaret.dataset\u001b[0m:\u001b[36mload_dataset\u001b[0m:\u001b[36m65\u001b[0m - \u001b[1mSaved on '/home/bengsoon/Projects/xcs224u_project/zeroqaret/datasets/trec-covid'\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a68f55b7b24f455d9f91253aa9914241",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/171332 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function tqdm.__del__ at 0x7fa0c2d7a0d0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bengsoon/conda/envs/xcs224/lib/python3.9/site-packages/tqdm/std.py\", line 1161, in __del__\n",
      "    def __del__(self):\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m datasets \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ds \u001b[38;5;129;01min\u001b[39;00m eval_list:\n\u001b[0;32m----> 5\u001b[0m     datasets[ds] \u001b[38;5;241m=\u001b[39m \u001b[43mbeir_datasets\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Projects/xcs224u_project/zeroqaret/zeroqaret/dataset.py:69\u001b[0m, in \u001b[0;36mBEIRDataset.load_dataset\u001b[0;34m(self, dataset, split)\u001b[0m\n\u001b[1;32m     65\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSaved on \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdatasets[dataset] \u001b[38;5;241m=\u001b[39m dataset_path\n\u001b[0;32m---> 69\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mGenericDataLoader\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdatasets\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/conda/envs/xcs224/lib/python3.9/site-packages/beir/datasets/data_loader.py:68\u001b[0m, in \u001b[0;36mGenericDataLoader.load\u001b[0;34m(self, split)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcorpus):\n\u001b[1;32m     67\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading Corpus...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 68\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_corpus\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     69\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoaded \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m Documents.\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcorpus), split\u001b[38;5;241m.\u001b[39mupper())\n\u001b[1;32m     70\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDoc Example: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcorpus\u001b[38;5;241m.\u001b[39mvalues())[\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[0;32m~/conda/envs/xcs224/lib/python3.9/site-packages/beir/datasets/data_loader.py:101\u001b[0m, in \u001b[0;36mGenericDataLoader._load_corpus\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcorpus_file, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf8\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m fIn:\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m tqdm(fIn, total\u001b[38;5;241m=\u001b[39mnum_lines):\n\u001b[0;32m--> 101\u001b[0m         line \u001b[38;5;241m=\u001b[39m \u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mline\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    102\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcorpus[line\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_id\u001b[39m\u001b[38;5;124m\"\u001b[39m)] \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    103\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m: line\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    104\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m\"\u001b[39m: line\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    105\u001b[0m         }\n",
      "File \u001b[0;32m~/conda/envs/xcs224/lib/python3.9/json/__init__.py:346\u001b[0m, in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    341\u001b[0m     s \u001b[38;5;241m=\u001b[39m s\u001b[38;5;241m.\u001b[39mdecode(detect_encoding(s), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msurrogatepass\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    344\u001b[0m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    345\u001b[0m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[0;32m--> 346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_decoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    348\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m JSONDecoder\n",
      "File \u001b[0;32m~/conda/envs/xcs224/lib/python3.9/json/decoder.py:337\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, s, _w\u001b[38;5;241m=\u001b[39mWHITESPACE\u001b[38;5;241m.\u001b[39mmatch):\n\u001b[1;32m    333\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[1;32m    334\u001b[0m \u001b[38;5;124;03m    containing a JSON document).\u001b[39;00m\n\u001b[1;32m    335\u001b[0m \n\u001b[1;32m    336\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 337\u001b[0m     obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraw_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_w\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    338\u001b[0m     end \u001b[38;5;241m=\u001b[39m _w(s, end)\u001b[38;5;241m.\u001b[39mend()\n\u001b[1;32m    339\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m end \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(s):\n",
      "File \u001b[0;32m~/conda/envs/xcs224/lib/python3.9/json/decoder.py:353\u001b[0m, in \u001b[0;36mJSONDecoder.raw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Decode a JSON document from ``s`` (a ``str`` beginning with\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;124;03ma JSON document) and return a 2-tuple of the Python\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;124;03mrepresentation and the index in ``s`` where the document ended.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    350\u001b[0m \n\u001b[1;32m    351\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 353\u001b[0m     obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscan_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    355\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpecting value\u001b[39m\u001b[38;5;124m\"\u001b[39m, s, err\u001b[38;5;241m.\u001b[39mvalue) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# download and store all datasets in a dictionary (datasets)\n",
    "datasets = {}\n",
    "\n",
    "for ds in eval_list:\n",
    "    datasets[ds] = beir_datasets.load_dataset(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## SBERT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from beir.retrieval import models\n",
    "from beir.retrieval.search.dense import DenseRetrievalExactSearch\n",
    "from beir.retrieval.evaluation import EvaluateRetrieval\n",
    "from beir import util\n",
    "from time import time\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sbert_model_name = \"all-mpnet-base-v2\"\n",
    "sbert_model = models.SentenceBERT(model_path=sbert_model_name)\n",
    "batch_size = 256,\n",
    "\n",
    "normalize = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sbert_model = DenseRetrievalExactSearch(models.SentenceBERT(sbert_model_name), batch_size = 256, corpus_chunk_size=512*9999)\n",
    "sbert_retriever = EvaluateRetrieval(sbert_model, score_function=\"dot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus, queries, qrels = datasets[\"fiqa\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time()\n",
    "sbert_results = sbert_retriever.retrieve(corpus, queries)\n",
    "end_time = time()\n",
    "print(\"Time taken to retrieve: {:.2f} seconds\".format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Format of `results` from `retriever.retrieve`:\n",
    "``` python\n",
    "    {\n",
    "        str(qid) : {\n",
    "            str(pid) : score\n",
    "        }\n",
    "    }\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"Retriever evaluation for k in: {}\".format(sbert_retriever.k_values))\n",
    "sbert_ndcg, sbert_map, sbert_recall, sbert_precision = sbert_retriever.evaluate(qrels, sbert_results, sbert_retriever.k_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sbert_ndcg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sbert_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sbert_recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sbert_precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mrr = retriever.evaluate_custom(qrels, results, retriever.k_values, metric=\"mrr\")\n",
    "recall_cap = retriever.evaluate_custom(qrels, results, retriever.k_values, metric=\"r_cap\")\n",
    "hole = retriever.evaluate_custom(qrels, results, retriever.k_values, metric=\"hole\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k = 10\n",
    "\n",
    "query_id, ranking_scores = random.choice(list(results.items()))\n",
    "scores_sorted = sorted(ranking_scores.items(), key=lambda item: item[1], reverse=True)\n",
    "logger.info(\"Query : %s\\n\" % queries[query_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for rank in range(top_k):\n",
    "    doc_id = scores_sorted[rank][0]\n",
    "    # Format: Rank x: ID [Title] Body\n",
    "    logger.info(\"Rank %d: %s [%s] - %s\\n\" % (rank+1, doc_id, corpus[doc_id].get(\"title\"), corpus[doc_id].get(\"text\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndcg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from beir.retrieval import models\n",
    "# from beir import util\n",
    "# from typing import Union, Tuple, List\n",
    "# from datetime import datetime\n",
    "# import torch\n",
    "# import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# logger.info(\"Computing Document Embeddings...\")\n",
    "# if normalize:\n",
    "#     corpus_embs = model.encode_corpus(reduced_corpus, batch_size=128, convert_to_tensor=True, normalize_embeddings=True)\n",
    "# else:\n",
    "#     corpus_embs = model.encode_corpus(reduced_corpus, batch_size=128, convert_to_tensor=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class SBERTEval(models.SentenceBERT):\n",
    "#     def __init__(self,\n",
    "#                  model_path: Union[str, Tuple] = None, \n",
    "#                  sep: str = \" \", # separator for corpus title and texts.\n",
    "#                  normalize: bool = True, # if True, normalize encodings. Use dot-product if normalize, otherwise cosine-sim.\n",
    "#                  encoding_batch_size: int = 128, # batch size for document embedding calculations.\n",
    "#                  eval_list: List[str] = [\"fiqa\", \"trec-covid\"], # list of the names of the BEIR datasets.\n",
    "#                  k_value: int = 10, # Top-k retrieval value for similarity search\n",
    "                 \n",
    "#                 ) -> None:\n",
    "#         \"\"\" \n",
    "#         Wrapper function for models.SentenceBERT with evaluation and experimentation functionality with MLflow. \n",
    "#         Adapted from https://github.com/beir-cellar/beir/blob/main/examples/benchmarking/benchmark_sbert.py\n",
    "#         \"\"\"\n",
    "#         super().__init__(model_path, sep)\n",
    "#         self.normalize = normalize\n",
    "#         self.encoding_batch_size = encoding_batch_size\n",
    "#         self.k_value = k_value\n",
    "\n",
    "#         ### LOAD Datasets ###\n",
    "#         # list of evaluation datasets\n",
    "#         self.eval_list = eval_list \n",
    "#         self.beir_datasets = BEIRDataset()\n",
    "#         # download and store all datasets in a dictionary (datasets)\n",
    "#         self.datasets = {ds: self.beir_datasets.load_dataset(ds) for ds in self.eval_list}\n",
    "#         logger.info(f\"BEIR datasets downloaded and loaded in self.datasets dictionary\")\n",
    "\n",
    "#     def eval_dataset(self,\n",
    "#                      dataset_name: str\n",
    "#                     ) -> None:\n",
    "\n",
    "#         logger.info(f\" Evaluation for '{dataset_name}' \".center(80, \"#\"))\n",
    "                    \n",
    "#         # load dataset\n",
    "#         corpus, queries, qrels = self.datasets[dataset_name]\n",
    "#         corpus_ids, query_ids = list(corpus), list(queries)\n",
    "\n",
    "#         corpus = [corpus[corpus_id] for corpus_id in corpus_ids]\n",
    "        \n",
    "#         # compute document embeddgs\n",
    "#         logger.info(\"Computing Document Embeddings...\")\n",
    "#         if self.normalize:\n",
    "#             self.corpus_embs = self.encode_corpus(corpus, batch_size=self.encoding_batch_size, convert_to_tensor=True, normalize_embeddings=True)\n",
    "#         else:\n",
    "#             self.corpus_embs = self.encode_corpus(corpus, batch_size=self.encoding_batch_size, convert_to_tensor=True)\n",
    "\n",
    "#         #### Saving benchmark times\n",
    "#         time_taken_all = {}\n",
    "        \n",
    "#         for query_id in query_ids:\n",
    "#             query = queries[query_id]\n",
    "            \n",
    "#             #### Compute query embedding and retrieve similar scores using dot-product\n",
    "#             start = datetime.now()\n",
    "#             if normalize:\n",
    "#                 query_emb = model.encode_queries([query], batch_size=1, convert_to_tensor=True, normalize_embeddings=True, show_progress_bar=False)\n",
    "#                 #### Dot product for normalized embeddings is equal to cosine similarity\n",
    "#                 sim_scores = util.dot_score(query_emb, corpus_embs)\n",
    "#             else:\n",
    "#                 query_emb = model.encode_queries([query], batch_size=1, convert_to_tensor=True, show_progress_bar=False)\n",
    "#                 #### Behind the hood, this cos_sim function will normalize the tensors first before applying dot-product \n",
    "#                 sim_scores = util.cos_sim(query_emb, corpus_embs)\n",
    "            \n",
    "#             #### Get top-k ranking\n",
    "#             sim_scores_top_k_values, sim_scores_top_k_idx = torch.topk(sim_scores, self.k_value, dim=1, largest=True, sorted=True)\n",
    "#             end = datetime.now()\n",
    "            \n",
    "#             #### Measuring time taken in ms (milliseconds)\n",
    "#             time_taken = (end - start)\n",
    "#             time_taken = time_taken.total_seconds() * 1000\n",
    "#             time_taken_all[query_id] = time_taken\n",
    "#             logger.info(\"{}: {} {:.2f}ms\".format(query_id, query, time_taken))\n",
    "\n",
    "#             time_taken = list(time_taken_all.values())\n",
    "#             logger.info(\"Average time taken: {:.2f}ms\".format(sum(time_taken)/len(time_taken_all)))\n",
    "            \n",
    "#             #### Measuring Index size consumed by document embeddings\n",
    "#             corpus_embs = corpus_embs.cpu()\n",
    "#             cpu_memory = sys.getsizeof(np.asarray([emb.numpy() for emb in corpus_embs]))\n",
    "            \n",
    "#             logging.info(\"Number of documents: {}, Dim: {}\".format(len(corpus_embs), len(corpus_embs[0])))\n",
    "#             logging.info(\"Index size (in MB): {:.2f}MB\".format(cpu_memory*0.000001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sbert = SBERTEval(sbert_model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## ColBERTv2 - vanilla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# faiss-gpu\n",
    "# %conda install -c pytorch -c nvidia faiss-gpu=1.7.4 mkl=2021 blas=1.0=mkl\n",
    "\n",
    "# torch\n",
    "# %pip install torch=1.13.1 torchaudio==0.13.1 torchvision==0.14.1\n",
    "\n",
    "# others\n",
    "# %pip install bitarray datasets gitpython ninja scipy spacy tqdm transformers ujson flask python-dotenv\n",
    "\n",
    "## git clone colbert repo into \"../ColBERT\"\n",
    "# !cd .. && git clone https://github.com/stanford-futuredata/ColBERT.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The original code in `load_collection()` from _ColBERT/colbert/evaluation/loaders.py_ required monotonic `pid`, but that is not necessarily our case. We'll have to monkey patch it to pass that assertion at line 166: ```assert pid == 'id' or int(pid) == line_idx, f\"pid={pid}, line_idx={line_idx}\"``` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"fiqa\"\n",
    "corpus, queries, qrels = beir_datasets.load_dataset(dataset_name)\n",
    "\n",
    "# The indices in BeIR datasets may not be monotic, \n",
    "### so we will need a dictionary with enumerated indices (which is used in ColBERT) as keys and BeIR index as values\n",
    "### collection_ids = {colbert_index: beir_index}\n",
    "collection_ids = {idx: val for idx, val in enumerate(list(corpus))}\n",
    "\n",
    "# Load datasets for ColBERT\n",
    "collection_path, queries_path = beir_datasets.convert_for_colbert(dataset_name)\n",
    "collection, queries = Collection(path=collection_path), Queries(path=queries_path)\n",
    "\n",
    "# queries_ids = list(queries)\n",
    "# queries = list(queries.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at an example of a query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries[8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and an example of a passage from the collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(collection[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbits = 2   # encode each dimension with 2 bits\n",
    "doc_maxlen = 300 # truncate passages at 300 tokens\n",
    "\n",
    "index_name = f'{dataset_name}.{nbits}bits'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = 'colbert-ir/colbertv2.0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with Run().context(RunConfig(nranks=1, experiment='notebook')):  # nranks specifies the number of GPUs to use\n",
    "    config = ColBERTConfig(doc_maxlen=doc_maxlen, nbits=nbits, kmeans_niters=4) # kmeans_niters specifies the number of iterations of k-means clustering; 4 is a good and fast default.\n",
    "                                                                                # Consider larger numbers for small datasets.\n",
    "\n",
    "    indexer = Indexer(checkpoint=checkpoint, config=config)\n",
    "    indexer.index(name=index_name, collection=collection, overwrite='reuse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexer.index??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexer.get_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To create the searcher using its relative name (i.e., not a full path), set\n",
    "# experiment=value_used_for_indexing in the RunConfig.\n",
    "with Run().context(RunConfig(experiment='notebook')):\n",
    "    searcher = Searcher(index=index_name, collection=collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = queries[8] # try with an in-range query or supply your own\n",
    "print(f\"#> {query}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the top-3 passages for this query\n",
    "results = searcher.search(query, k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qrels['8']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out the top-k retrieved passages\n",
    "for passage_id, passage_rank, passage_score in zip(*results):\n",
    "    print(f\"\\t [{passage_rank}] \\t\\t {passage_score:.1f} \\t\\t {searcher.collection[passage_id]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rankings = searcher.search_all(queries, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rankings.todict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ColBERTv2 as BeIR Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"fiqa\"\n",
    "corpus, queries, qrels = beir_datasets.load_dataset(dataset_name)\n",
    "\n",
    "# The indices in BeIR datasets may not be monotic, \n",
    "### so we will need a dictionary with enumerated indices (which is used in ColBERT) as keys and BeIR index as values\n",
    "### collection_ids = {colbert_index: beir_index}\n",
    "collection_ids = {idx: str(val) for idx, val in enumerate(list(corpus))}\n",
    "\n",
    "# Load datasets for ColBERT\n",
    "collection_path, queries_path = beir_datasets.convert_for_colbert(dataset_name)\n",
    "collection, queries = Collection(path=collection_path), Queries(path=queries_path)\n",
    "\n",
    "# queries_ids = list(queries)\n",
    "# queries = list(queries.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbits = 2   # encode each dimension with 2 bits\n",
    "doc_maxlen = 300 # truncate passages at 300 tokens\n",
    "\n",
    "index_name = f'{dataset_name}.{nbits}bits'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class ColBERTRetrievalSearch(Indexer):\n",
    "    def __init__(self, \n",
    "                 checkpoint: str, # ColBERT checkpoint\n",
    "                 index_name: str, # name of the index\n",
    "                 experiment_name: str, # name of experiment\n",
    "                 collection: \"Collection\", # collection object in Collection format\n",
    "                 collection_ids: Dict, # {colbert_index: beir_pid}\n",
    "                 doc_maxlen: int,\n",
    "                 nbits: int,\n",
    "                 kmeans: int = 4,\n",
    "                 overwrite_param: Union[bool, str] = 'reuse',\n",
    "                 **kwargs):\n",
    "        \"\"\"\n",
    "        Retrieval Search wrapper for ColBERTv2, adapted from BeIR's `DenseRetrievalExactSearch`\n",
    "         (https://github.com/beir-cellar/beir/blob/f062f038c4bfd19a8ca942a9910b1e0d218759d4/beir/retrieval/search/dense/exact_search.py#L12).\n",
    "\n",
    "        The difference to BeIR's implementation is that if `corpus` and `corpus_ids` are passed at initialization stage, \n",
    "            it will pre-compute document encodings and store it. \n",
    "\n",
    "        If `index_name` and `overwrite = 'reuse'        \n",
    "        \"\"\"\n",
    "        self.checkpoint = checkpoint\n",
    "        self.index_name = index_name\n",
    "        self.collection = collection\n",
    "        self.collection_ids = collection_ids\n",
    "        self.experiment_name = experiment_name\n",
    "        self.doc_maxlen = doc_maxlen\n",
    "        self.nbits = nbits\n",
    "        self.kmeans = kmeans\n",
    "        self.overwrite_param = overwrite_param\n",
    "        \n",
    "        with Run().context(RunConfig(nranks=1, experiment=experiment_name)):  # nranks specifies the number of GPUs to use\n",
    "            config = ColBERTConfig(doc_maxlen=self.doc_maxlen, nbits=self.nbits, kmeans_niters=self.kmeans) # kmeans_niters specifies the number of iterations of k-means clustering; 4 is a good and fast default.\n",
    "                                                                                        # Consider larger numbers for small datasets.\n",
    "        \n",
    "            super().__init__(checkpoint=self.checkpoint, config=config)\n",
    "            self.index(name=self.index_name, collection=self.collection, overwrite=self.overwrite_param)\n",
    "            \n",
    "            self.searcher = Searcher(index=self.index_name, collection=self.collection)\n",
    "\n",
    "    def search(self,\n",
    "               corpus: \"Collection\" = None, # corpus in Collection format\n",
    "               queries: \"Queries\" = None, # queries in Queries format\n",
    "               k: int = 10, # top-K value\n",
    "               score_function = None, # redundant; here to make it compatible with function call from EvaluateRetrieval\n",
    "               filter_fn = None,              \n",
    "               full_length_search: bool = False,\n",
    "               **kwargs,\n",
    "              ) -> Dict[str, Dict[str, float]]:\n",
    "\n",
    "        res = self.searcher.search_all(queries, k, filter_fn, full_length_search)\n",
    "        self.results = {}\n",
    "        for qid, doc_res in res.items():\n",
    "            doc_res = {self.collection_ids[cid] : score for cid, rank, score in doc_res}\n",
    "            self.results[str(qid)] = doc_res\n",
    "\n",
    "        return self.results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ColBERTRetrievalSearch(checkpoint, \n",
    "                                   index_name, \n",
    "                                   experiment_name=\"ColBERTRetrievalSearch_test\", \n",
    "                                   collection=collection, \n",
    "                                   collection_ids=collection_ids,\n",
    "                                   doc_maxlen=doc_maxlen, \n",
    "                                   nbits=nbits, \n",
    "                                   overwrite_param=\"reuse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = EvaluateRetrieval(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = retriever.retrieve(collection, queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndcg, _map, recall, precision = retriever.evaluate(qrels, results, retriever.k_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndcg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results Collector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.DataFrame()\n",
    "test_df[\"test\"] = pd.Series(ndcg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class ResultsCollector:\n",
    "    \"\"\" Collect results from Retrieval Evaluation \"\"\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def evaluate(self,\n",
    "                 model_name,\n",
    "                 retriever,\n",
    "                 qrels, \n",
    "                 results):\n",
    "        \n",
    "        ndcg, map, recall, precision = retriever.evaluate(qrels, results, retriever.k_values)\n",
    "        if not hasattr(self, \"ndcg\"): self.ndcg = pd.DataFrame()\n",
    "        if not hasattr(self, \"map\"): self.map = pd.DataFrame()\n",
    "        if not hasattr(self, \"recall\"): self.recall = pd.DataFrame()\n",
    "        if not hasattr(self, \"precision\"): self.precision = pd.DataFrame()\n",
    "        \n",
    "        self.ndcg[model_name] = pd.Series(ndcg)\n",
    "        self.map[model_name] = pd.Series(map)\n",
    "        self.recall[model_name] = pd.Series(recall)\n",
    "        self.precision[model_name] = pd.Series(precision)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_collector = ResultsCollector()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xcs224u",
   "language": "python",
   "name": "xcs224u"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
