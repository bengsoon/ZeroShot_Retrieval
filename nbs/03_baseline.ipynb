{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline\n",
    "\n",
    "> Baseline Evaluation: Baseline evaluation for zeroqaret project. The evaluation modules here will also be re-used for other evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bengsoon/conda/envs/xcs224/lib/python3.9/site-packages/beir/datasets/data_loader.py:2: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "from loguru import logger\n",
    "import os\n",
    "from pathlib import Path\n",
    "from fastcore.basics import patch_to, patch\n",
    "\n",
    "from zeroqaret.dataset import BEIRDataset, our_list as eval_list\n",
    "from zeroqaret.evaluation import ColBERTRetrievalSearch, ResultsCollector\n",
    "\n",
    "from beir.retrieval import models\n",
    "from beir.retrieval.search.dense import DenseRetrievalExactSearch\n",
    "from beir.retrieval.evaluation import EvaluateRetrieval\n",
    "from beir import util\n",
    "from time import time\n",
    "\n",
    "from colbert import Indexer, Searcher\n",
    "from colbert.infra import Run, RunConfig, ColBERTConfig\n",
    "from colbert.data import Queries, Collection\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fiqa', 'trec-covid']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# these are the list of datasets to be evaluated\n",
    "eval_list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-10-26 17:26:32.422\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mzeroqaret.dataset\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m51\u001b[0m - \u001b[1mDatasets will be saved in '/home/bengsoon/Projects/xcs224u_project/zeroqaret/datasets'\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "beir_datasets = BEIRDataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FIQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14b8e15f515643199eaa73a4e130f089",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/57638 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a72c38864777412fbbe140858f522b96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/57638 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-10-26 16:12:36.997\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mzeroqaret.dataset\u001b[0m:\u001b[36mconvert_for_colbert\u001b[0m:\u001b[36m98\u001b[0m - \u001b[1mPreprocessing Corpus and Saving to /home/bengsoon/Projects/xcs224u_project/zeroqaret/datasets/fiqa/colbert/fiqa_collection.tsv ...\u001b[0m\n",
      "100%|████| 57638/57638 [00:00<00:00, 66117.34it/s]\n",
      "\u001b[32m2023-10-26 16:12:37.884\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mzeroqaret.dataset\u001b[0m:\u001b[36mconvert_for_colbert\u001b[0m:\u001b[36m105\u001b[0m - \u001b[1mPreprocessing Corpus and Saving to /home/bengsoon/Projects/xcs224u_project/zeroqaret/datasets/fiqa/colbert/fiqa_queries.tsv ...\u001b[0m\n",
      "100%|███████| 648/648 [00:00<00:00, 487604.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Oct 26, 16:12:37] #> Loading collection...\n",
      "0M "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Oct 26, 16:12:38] #> Loading the queries from /home/bengsoon/Projects/xcs224u_project/zeroqaret/datasets/fiqa/colbert/fiqa_queries.tsv ...\n",
      "[Oct 26, 16:12:38] #> Got 648 queries. All QIDs are unique.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset_name = \"fiqa\"\n",
    "corpus, queries, qrels = beir_datasets.load_dataset(dataset_name)\n",
    "\n",
    "# The indices in BeIR datasets may not be monotic, \n",
    "### so we will need a dictionary with enumerated indices (which is used in ColBERT) as keys and BeIR index as values\n",
    "### collection_ids = {colbert_index: beir_index}\n",
    "collection_ids = {idx: str(val) for idx, val in enumerate(list(corpus))}\n",
    "\n",
    "# Load datasets for ColBERT\n",
    "collection_path, queries_path = beir_datasets.convert_for_colbert(dataset_name)\n",
    "colbert_collection, colbert_queries = Collection(path=collection_path), Queries(path=queries_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Baseline SBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "sbert_model_name = \"all-mpnet-base-v2\"\n",
    "sbert_model = models.SentenceBERT(model_path=sbert_model_name)\n",
    "batch_size = 256,\n",
    "normalize = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "sbert_model = DenseRetrievalExactSearch(models.SentenceBERT(sbert_model_name), batch_size = 256, corpus_chunk_size=512*9999)\n",
    "sbert_retriever = EvaluateRetrieval(sbert_model, score_function=\"dot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8137738255de44738b4aa3438a7e880b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c9f42f3de744ad4b565708b8c006f39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/226 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to retrieve: 284.14 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time()\n",
    "sbert_results = sbert_retriever.retrieve(corpus, queries)\n",
    "end_time = time()\n",
    "print(\"Time taken to retrieve: {:.2f} seconds\".format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_collector = ResultsCollector()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_collector.evaluate(\"SBERT Baseline\", sbert_retriever, qrels, sbert_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SBERT Baseline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>NDCG@1</th>\n",
       "      <td>0.49074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NDCG@3</th>\n",
       "      <td>0.45489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NDCG@5</th>\n",
       "      <td>0.47133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NDCG@10</th>\n",
       "      <td>0.49963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NDCG@100</th>\n",
       "      <td>0.56564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NDCG@1000</th>\n",
       "      <td>0.58932</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           SBERT Baseline\n",
       "NDCG@1            0.49074\n",
       "NDCG@3            0.45489\n",
       "NDCG@5            0.47133\n",
       "NDCG@10           0.49963\n",
       "NDCG@100          0.56564\n",
       "NDCG@1000         0.58932"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_collector.ndcg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Baseline ColBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = 'colbert-ir/colbertv2.0'\n",
    "nbits = 2   # encode each dimension with 2 bits\n",
    "doc_maxlen = 300 # truncate passages at 300 tokens\n",
    "\n",
    "experiment_name = 'baseline'\n",
    "index_name = f'{experiment_name}_{dataset_name}.{nbits}bits'\n",
    "kmeans=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "[Oct 26, 16:59:17] #> Creating directory /home/bengsoon/Projects/xcs224u_project/zeroqaret/nbs/experiments/notebook/indexes/baseline_fiqa.2bits \n",
      "\n",
      "\n",
      "#> Starting...\n",
      "nranks = 1 \t num_gpus = 1 \t device=0\n",
      "{\n",
      "    \"query_token_id\": \"[unused0]\",\n",
      "    \"doc_token_id\": \"[unused1]\",\n",
      "    \"query_token\": \"[Q]\",\n",
      "    \"doc_token\": \"[D]\",\n",
      "    \"ncells\": null,\n",
      "    \"centroid_score_threshold\": null,\n",
      "    \"ndocs\": null,\n",
      "    \"load_index_with_mmap\": false,\n",
      "    \"index_path\": null,\n",
      "    \"nbits\": 2,\n",
      "    \"kmeans_niters\": 4,\n",
      "    \"resume\": false,\n",
      "    \"similarity\": \"cosine\",\n",
      "    \"bsize\": 64,\n",
      "    \"accumsteps\": 1,\n",
      "    \"lr\": 3e-6,\n",
      "    \"maxsteps\": 500000,\n",
      "    \"save_every\": null,\n",
      "    \"warmup\": null,\n",
      "    \"warmup_bert\": null,\n",
      "    \"relu\": false,\n",
      "    \"nway\": 2,\n",
      "    \"use_ib_negatives\": false,\n",
      "    \"reranker\": false,\n",
      "    \"distillation_alpha\": 1.0,\n",
      "    \"ignore_scores\": false,\n",
      "    \"model_name\": null,\n",
      "    \"query_maxlen\": 32,\n",
      "    \"attend_to_mask_tokens\": false,\n",
      "    \"interaction\": \"colbert\",\n",
      "    \"dim\": 128,\n",
      "    \"doc_maxlen\": 300,\n",
      "    \"mask_punctuation\": true,\n",
      "    \"checkpoint\": \"colbert-ir\\/colbertv2.0\",\n",
      "    \"triples\": null,\n",
      "    \"collection\": \"\\/home\\/bengsoon\\/Projects\\/xcs224u_project\\/zeroqaret\\/datasets\\/fiqa\\/colbert\\/fiqa_collection.tsv\",\n",
      "    \"queries\": null,\n",
      "    \"index_name\": \"baseline_fiqa.2bits\",\n",
      "    \"overwrite\": false,\n",
      "    \"root\": \"\\/home\\/bengsoon\\/Projects\\/xcs224u_project\\/zeroqaret\\/nbs\\/experiments\",\n",
      "    \"experiment\": \"notebook\",\n",
      "    \"index_root\": null,\n",
      "    \"name\": \"2023-10\\/26\\/16.05.25\",\n",
      "    \"rank\": 0,\n",
      "    \"nranks\": 1,\n",
      "    \"amp\": true,\n",
      "    \"gpus\": 1\n",
      "}\n",
      "[Oct 26, 16:59:24] [0] \t\t # of sampled PIDs = 42079 \t sampled_pids[:3] = [27303, 48017, 666]\n",
      "[Oct 26, 16:59:24] [0] \t\t #> Encoding 42079 passages..\n",
      "[Oct 26, 17:00:55] [0] \t\t avg_doclen_est = 129.19097900390625 \t len(local_sample) = 42,079\n",
      "[Oct 26, 17:00:56] [0] \t\t Creaing 32,768 partitions.\n",
      "[Oct 26, 17:00:56] [0] \t\t *Estimated* 7,446,309 embeddings.\n",
      "[Oct 26, 17:00:56] [0] \t\t #> Saving the indexing plan to /home/bengsoon/Projects/xcs224u_project/zeroqaret/nbs/experiments/notebook/indexes/baseline_fiqa.2bits/plan.json ..\n",
      "Clustering 5386227 points in 128D to 32768 clusters, redo 1 times, 4 iterations\n",
      "  Preprocessing in 0.36 s\n",
      "  Iteration 3 (22.41 s, search 21.54 s): objective=1.47235e+06 imbalance=1.328 nsplit=0       \n",
      "[Oct 26, 17:01:21] Loading decompress_residuals_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\n",
      "[Oct 26, 17:01:21] Loading packbits_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\n",
      "[0.037, 0.038, 0.039, 0.035, 0.039, 0.038, 0.037, 0.035, 0.036, 0.038, 0.035, 0.037, 0.039, 0.04, 0.037, 0.04, 0.034, 0.035, 0.037, 0.035, 0.039, 0.038, 0.035, 0.038, 0.034, 0.035, 0.037, 0.036, 0.039, 0.041, 0.038, 0.039, 0.04, 0.035, 0.035, 0.034, 0.037, 0.036, 0.038, 0.043, 0.037, 0.035, 0.036, 0.037, 0.036, 0.038, 0.035, 0.04, 0.04, 0.037, 0.036, 0.036, 0.039, 0.036, 0.035, 0.036, 0.041, 0.041, 0.047, 0.035, 0.035, 0.039, 0.036, 0.039, 0.039, 0.038, 0.041, 0.039, 0.036, 0.037, 0.039, 0.033, 0.036, 0.039, 0.036, 0.038, 0.038, 0.04, 0.039, 0.039, 0.039, 0.037, 0.036, 0.037, 0.036, 0.036, 0.037, 0.036, 0.037, 0.043, 0.038, 0.039, 0.036, 0.038, 0.036, 0.037, 0.041, 0.035, 0.037, 0.037, 0.036, 0.039, 0.036, 0.038, 0.04, 0.037, 0.036, 0.036, 0.036, 0.036, 0.037, 0.037, 0.038, 0.039, 0.037, 0.038, 0.041, 0.04, 0.036, 0.039, 0.035, 0.038, 0.038, 0.038, 0.035, 0.038, 0.039, 0.037]\n",
      "[Oct 26, 17:01:22] #> Got bucket_cutoffs_quantiles = tensor([0.2500, 0.5000, 0.7500], device='cuda:0') and bucket_weights_quantiles = tensor([0.1250, 0.3750, 0.6250, 0.8750], device='cuda:0')\n",
      "[Oct 26, 17:01:22] #> Got bucket_cutoffs = tensor([-0.0301,  0.0001,  0.0303], device='cuda:0') and bucket_weights = tensor([-0.0529, -0.0139,  0.0142,  0.0533], device='cuda:0')\n",
      "[Oct 26, 17:01:22] avg_residual = 0.0374755859375\n",
      "[Oct 26, 17:01:22] [0] \t\t #> Encoding 25000 passages..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Oct 26, 17:02:16] [0] \t\t #> Saving chunk 0: \t 25,000 passages and 3,215,621 embeddings. From #0 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:55, 55.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Oct 26, 17:02:17] [0] \t\t #> Encoding 25000 passages..\n",
      "[Oct 26, 17:03:12] [0] \t\t #> Saving chunk 1: \t 25,000 passages and 3,246,215 embeddings. From #25,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [01:51, 55.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Oct 26, 17:03:13] [0] \t\t #> Encoding 7638 passages..\n",
      "[Oct 26, 17:03:30] [0] \t\t #> Saving chunk 2: \t 7,638 passages and 975,896 embeddings. From #50,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [02:08, 42.81s/it]\n",
      "100%|██████████| 3/3 [00:00<00:00, 285.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Oct 26, 17:03:30] [0] \t\t #> Checking all files were saved...\n",
      "[Oct 26, 17:03:30] [0] \t\t Found all files!\n",
      "[Oct 26, 17:03:30] [0] \t\t #> Building IVF...\n",
      "[Oct 26, 17:03:30] [0] \t\t #> Loading codes...\n",
      "[Oct 26, 17:03:30] [0] \t\t Sorting codes...\n",
      "[Oct 26, 17:03:31] [0] \t\t Getting unique codes...\n",
      "[Oct 26, 17:03:31] #> Optimizing IVF to store map from centroids to list of pids..\n",
      "[Oct 26, 17:03:31] #> Building the emb2pid mapping..\n",
      "[Oct 26, 17:03:31] len(emb2pid) = 7437732\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32768/32768 [00:00<00:00, 40875.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Oct 26, 17:03:32] #> Saved optimized IVF to /home/bengsoon/Projects/xcs224u_project/zeroqaret/nbs/experiments/notebook/indexes/baseline_fiqa.2bits/ivf.pid.pt\n",
      "[Oct 26, 17:03:32] [0] \t\t #> Saving the indexing metadata to /home/bengsoon/Projects/xcs224u_project/zeroqaret/nbs/experiments/notebook/indexes/baseline_fiqa.2bits/metadata.json ..\n",
      "#> Joined...\n",
      "[Oct 26, 17:03:37] #> Loading codec...\n",
      "[Oct 26, 17:03:37] Loading decompress_residuals_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\n",
      "[Oct 26, 17:03:37] Loading packbits_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\n",
      "[Oct 26, 17:03:37] #> Loading IVF...\n",
      "[Oct 26, 17:03:38] #> Loading doclens...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████| 3/3 [00:00<00:00, 808.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Oct 26, 17:03:38] #> Loading codes and residuals...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|███████████████| 3/3 [00:00<00:00, 17.86it/s]\n"
     ]
    }
   ],
   "source": [
    "colbert_model = ColBERTRetrievalSearch(\n",
    "                                checkpoint, \n",
    "                                index_name, \n",
    "                                experiment_name, \n",
    "                                colbert_collection, \n",
    "                                collection_ids, \n",
    "                                doc_maxlen, \n",
    "                                nbits, \n",
    "                                kmeans, \n",
    "                                overwrite_param=True\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "colbert_retriever = EvaluateRetrieval(colbert_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "648it [00:08, 75.66it/s]\n"
     ]
    }
   ],
   "source": [
    "colbert_results = colbert_retriever.retrieve(colbert_collection, colbert_queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_collector.evaluate(\"ColBERT Baseline\", colbert_retriever, qrels, colbert_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SBERT Baseline</th>\n",
       "      <th>ColBERT Baseline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>NDCG@1</th>\n",
       "      <td>0.49074</td>\n",
       "      <td>0.33796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NDCG@3</th>\n",
       "      <td>0.45489</td>\n",
       "      <td>0.31461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NDCG@5</th>\n",
       "      <td>0.47133</td>\n",
       "      <td>0.32837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NDCG@10</th>\n",
       "      <td>0.49963</td>\n",
       "      <td>0.35277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NDCG@100</th>\n",
       "      <td>0.56564</td>\n",
       "      <td>0.41278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NDCG@1000</th>\n",
       "      <td>0.58932</td>\n",
       "      <td>0.44200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           SBERT Baseline  ColBERT Baseline\n",
       "NDCG@1            0.49074           0.33796\n",
       "NDCG@3            0.45489           0.31461\n",
       "NDCG@5            0.47133           0.32837\n",
       "NDCG@10           0.49963           0.35277\n",
       "NDCG@100          0.56564           0.41278\n",
       "NDCG@1000         0.58932           0.44200"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_collector.ndcg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Results for FIQA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TREC-Covid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-10-26 17:26:34.698\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mzeroqaret.dataset\u001b[0m:\u001b[36mload_dataset\u001b[0m:\u001b[36m62\u001b[0m - \u001b[1mDownloading dataset 'trec-covid'...\u001b[0m\n",
      "\u001b[32m2023-10-26 17:26:34.701\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mzeroqaret.dataset\u001b[0m:\u001b[36mload_dataset\u001b[0m:\u001b[36m65\u001b[0m - \u001b[1mSaved on '/home/bengsoon/Projects/xcs224u_project/zeroqaret/datasets/trec-covid'\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27115b710cbf47ac968b8c375e81c378",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/171332 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89ced8bc30c14a77b732062131673ec7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/171332 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-10-26 17:26:37.782\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mzeroqaret.dataset\u001b[0m:\u001b[36mconvert_for_colbert\u001b[0m:\u001b[36m98\u001b[0m - \u001b[1mPreprocessing Corpus and Saving to /home/bengsoon/Projects/xcs224u_project/zeroqaret/datasets/trec-covid/colbert/trec-covid_collection.tsv ...\u001b[0m\n",
      "100%|██| 171332/171332 [00:03<00:00, 44269.15it/s]\n",
      "\u001b[32m2023-10-26 17:26:41.714\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mzeroqaret.dataset\u001b[0m:\u001b[36mconvert_for_colbert\u001b[0m:\u001b[36m105\u001b[0m - \u001b[1mPreprocessing Corpus and Saving to /home/bengsoon/Projects/xcs224u_project/zeroqaret/datasets/trec-covid/colbert/trec-covid_queries.tsv ...\u001b[0m\n",
      "100%|█████████| 50/50 [00:00<00:00, 300021.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Oct 26, 17:26:41] #> Loading collection...\n",
      "0M \n",
      "[Oct 26, 17:26:42] #> Loading the queries from /home/bengsoon/Projects/xcs224u_project/zeroqaret/datasets/trec-covid/colbert/trec-covid_queries.tsv ...\n",
      "[Oct 26, 17:26:42] #> Got 50 queries. All QIDs are unique.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset_name = \"trec-covid\"\n",
    "trec_corpus, trec_queries, trec_qrels = beir_datasets.load_dataset(dataset_name)\n",
    "\n",
    "# The indices in BeIR datasets may not be monotic, \n",
    "### so we will need a dictionary with enumerated indices (which is used in ColBERT) as keys and BeIR index as values\n",
    "### collection_ids = {colbert_index: beir_index}\n",
    "trec_collection_ids = {idx: str(val) for idx, val in enumerate(list(trec_corpus))}\n",
    "\n",
    "# Load datasets for ColBERT\n",
    "trec_collection_path, trec_queries_path = beir_datasets.convert_for_colbert(dataset_name)\n",
    "trec_colbert_collection, trec_colbert_queries = Collection(path=trec_collection_path), Queries(path=trec_queries_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline SBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sbert_model_name = \"all-mpnet-base-v2\"\n",
    "sbert_model = models.SentenceBERT(model_path=sbert_model_name)\n",
    "batch_size = 256,\n",
    "normalize = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sbert_model = DenseRetrievalExactSearch(models.SentenceBERT(sbert_model_name), batch_size = 128, corpus_chunk_size=512*9999)\n",
    "sbert_retriever = EvaluateRetrieval(sbert_model, score_function=\"dot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "babab1d56d3e451d93ac8490d2c532cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ddc0d311a064f46ac48ec93516be078",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1339 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to retrieve: 985.40 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time()\n",
    "sbert_trec_results = sbert_retriever.retrieve(trec_corpus, trec_queries)\n",
    "end_time = time()\n",
    "print(\"Time taken to retrieve: {:.2f} seconds\".format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastcore.utils import save_pickle\n",
    "\n",
    "save_pickle(f\"../datasets/{dataset_name}/results/sbert_trec_results.pkl\",  sbert_trec_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "trec_results_collector = ResultsCollector()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "trec_results_collector.evaluate(\"SBERT Baseline\", sbert_retriever, trec_qrels, sbert_trec_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SBERT Baseline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>MAP@1</th>\n",
       "      <td>0.00195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MAP@3</th>\n",
       "      <td>0.00465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MAP@5</th>\n",
       "      <td>0.00672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MAP@10</th>\n",
       "      <td>0.01218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MAP@100</th>\n",
       "      <td>0.07061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MAP@1000</th>\n",
       "      <td>0.19734</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          SBERT Baseline\n",
       "MAP@1            0.00195\n",
       "MAP@3            0.00465\n",
       "MAP@5            0.00672\n",
       "MAP@10           0.01218\n",
       "MAP@100          0.07061\n",
       "MAP@1000         0.19734"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trec_results_collector.map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline ColBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = 'colbert-ir/colbertv2.0'\n",
    "nbits = 2   # encode each dimension with 2 bits\n",
    "doc_maxlen = 300 # truncate passages at 300 tokens\n",
    "\n",
    "experiment_name = 'baseline'\n",
    "index_name = f'{experiment_name}_{dataset_name}.{nbits}bits'\n",
    "kmeans=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "[Oct 26, 17:49:00] #> Creating directory /home/bengsoon/Projects/xcs224u_project/zeroqaret/nbs/experiments/notebook/indexes/baseline_trec-covid.2bits \n",
      "\n",
      "\n",
      "#> Starting...\n",
      "nranks = 1 \t num_gpus = 1 \t device=0\n",
      "{\n",
      "    \"query_token_id\": \"[unused0]\",\n",
      "    \"doc_token_id\": \"[unused1]\",\n",
      "    \"query_token\": \"[Q]\",\n",
      "    \"doc_token\": \"[D]\",\n",
      "    \"ncells\": null,\n",
      "    \"centroid_score_threshold\": null,\n",
      "    \"ndocs\": null,\n",
      "    \"load_index_with_mmap\": false,\n",
      "    \"index_path\": null,\n",
      "    \"nbits\": 2,\n",
      "    \"kmeans_niters\": 4,\n",
      "    \"resume\": false,\n",
      "    \"similarity\": \"cosine\",\n",
      "    \"bsize\": 64,\n",
      "    \"accumsteps\": 1,\n",
      "    \"lr\": 3e-6,\n",
      "    \"maxsteps\": 500000,\n",
      "    \"save_every\": null,\n",
      "    \"warmup\": null,\n",
      "    \"warmup_bert\": null,\n",
      "    \"relu\": false,\n",
      "    \"nway\": 2,\n",
      "    \"use_ib_negatives\": false,\n",
      "    \"reranker\": false,\n",
      "    \"distillation_alpha\": 1.0,\n",
      "    \"ignore_scores\": false,\n",
      "    \"model_name\": null,\n",
      "    \"query_maxlen\": 32,\n",
      "    \"attend_to_mask_tokens\": false,\n",
      "    \"interaction\": \"colbert\",\n",
      "    \"dim\": 128,\n",
      "    \"doc_maxlen\": 300,\n",
      "    \"mask_punctuation\": true,\n",
      "    \"checkpoint\": \"colbert-ir\\/colbertv2.0\",\n",
      "    \"triples\": null,\n",
      "    \"collection\": \"\\/home\\/bengsoon\\/Projects\\/xcs224u_project\\/zeroqaret\\/datasets\\/trec-covid\\/colbert\\/trec-covid_collection.tsv\",\n",
      "    \"queries\": null,\n",
      "    \"index_name\": \"baseline_trec-covid.2bits\",\n",
      "    \"overwrite\": false,\n",
      "    \"root\": \"\\/home\\/bengsoon\\/Projects\\/xcs224u_project\\/zeroqaret\\/nbs\\/experiments\",\n",
      "    \"experiment\": \"notebook\",\n",
      "    \"index_root\": null,\n",
      "    \"name\": \"2023-10\\/26\\/17.26.30\",\n",
      "    \"rank\": 0,\n",
      "    \"nranks\": 1,\n",
      "    \"amp\": true,\n",
      "    \"gpus\": 1\n",
      "}\n",
      "[Oct 26, 17:49:08] [0] \t\t # of sampled PIDs = 72549 \t sampled_pids[:3] = [109214, 2665, 78286]\n",
      "[Oct 26, 17:49:09] [0] \t\t #> Encoding 72549 passages..\n",
      "[Oct 26, 17:51:50] [0] \t\t avg_doclen_est = 174.52500915527344 \t len(local_sample) = 72,549\n",
      "[Oct 26, 17:51:53] [0] \t\t Creaing 65,536 partitions.\n",
      "[Oct 26, 17:51:53] [0] \t\t *Estimated* 29,901,718 embeddings.\n",
      "[Oct 26, 17:51:53] [0] \t\t #> Saving the indexing plan to /home/bengsoon/Projects/xcs224u_project/zeroqaret/nbs/experiments/notebook/indexes/baseline_trec-covid.2bits/plan.json ..\n",
      "Clustering 12611615 points in 128D to 65536 clusters, redo 1 times, 4 iterations\n",
      "  Preprocessing in 1.38 s\n",
      "  Iteration 3 (108.64 s, search 99.52 s): objective=2.35291e+06 imbalance=1.336 nsplit=1       \n",
      "[Oct 26, 17:53:47] Loading decompress_residuals_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\n",
      "[Oct 26, 17:53:48] Loading packbits_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\n",
      "[0.029, 0.031, 0.032, 0.028, 0.029, 0.031, 0.032, 0.028, 0.029, 0.03, 0.031, 0.029, 0.031, 0.031, 0.028, 0.031, 0.027, 0.028, 0.028, 0.028, 0.031, 0.03, 0.029, 0.031, 0.029, 0.028, 0.031, 0.03, 0.029, 0.032, 0.029, 0.033, 0.031, 0.028, 0.029, 0.027, 0.031, 0.03, 0.029, 0.035, 0.031, 0.03, 0.03, 0.03, 0.029, 0.031, 0.029, 0.033, 0.031, 0.028, 0.028, 0.03, 0.031, 0.029, 0.03, 0.029, 0.035, 0.031, 0.035, 0.03, 0.029, 0.032, 0.03, 0.032, 0.031, 0.03, 0.031, 0.032, 0.029, 0.03, 0.031, 0.027, 0.029, 0.032, 0.029, 0.031, 0.032, 0.03, 0.031, 0.032, 0.032, 0.029, 0.03, 0.032, 0.028, 0.03, 0.03, 0.031, 0.028, 0.032, 0.03, 0.032, 0.03, 0.031, 0.03, 0.031, 0.034, 0.029, 0.029, 0.03, 0.031, 0.033, 0.032, 0.029, 0.031, 0.03, 0.031, 0.029, 0.031, 0.029, 0.031, 0.031, 0.031, 0.028, 0.03, 0.029, 0.03, 0.03, 0.029, 0.03, 0.029, 0.03, 0.032, 0.031, 0.028, 0.032, 0.03, 0.029]\n",
      "[Oct 26, 17:53:48] #> Got bucket_cutoffs_quantiles = tensor([0.2500, 0.5000, 0.7500], device='cuda:0') and bucket_weights_quantiles = tensor([0.1250, 0.3750, 0.6250, 0.8750], device='cuda:0')\n",
      "[Oct 26, 17:53:48] #> Got bucket_cutoffs = tensor([-2.3987e-02, -1.5259e-05,  2.3987e-02], device='cuda:0') and bucket_weights = tensor([-0.0425, -0.0111,  0.0111,  0.0427], device='cuda:0')\n",
      "[Oct 26, 17:53:48] avg_residual = 0.0301666259765625\n",
      "[Oct 26, 17:53:49] [0] \t\t #> Encoding 25000 passages..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Oct 26, 17:54:44] [0] \t\t #> Saving chunk 0: \t 25,000 passages and 3,741,686 embeddings. From #0 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:57, 57.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Oct 26, 17:54:46] [0] \t\t #> Encoding 25000 passages..\n",
      "[Oct 26, 17:55:42] [0] \t\t #> Saving chunk 1: \t 25,000 passages and 4,776,533 embeddings. From #25,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [01:56, 58.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Oct 26, 17:55:46] [0] \t\t #> Encoding 25000 passages..\n",
      "[Oct 26, 17:56:42] [0] \t\t #> Saving chunk 2: \t 25,000 passages and 4,964,069 embeddings. From #50,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [02:56, 58.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Oct 26, 17:56:45] [0] \t\t #> Encoding 25000 passages..\n",
      "[Oct 26, 17:57:40] [0] \t\t #> Saving chunk 3: \t 25,000 passages and 3,995,756 embeddings. From #75,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [03:54, 58.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Oct 26, 17:57:43] [0] \t\t #> Encoding 25000 passages..\n",
      "[Oct 26, 17:58:38] [0] \t\t #> Saving chunk 4: \t 25,000 passages and 3,455,785 embeddings. From #100,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [04:51, 58.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Oct 26, 17:58:40] [0] \t\t #> Encoding 25000 passages..\n",
      "[Oct 26, 17:59:36] [0] \t\t #> Saving chunk 5: \t 25,000 passages and 4,875,300 embeddings. From #125,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6it [05:50, 58.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Oct 26, 17:59:39] [0] \t\t #> Encoding 21332 passages..\n",
      "[Oct 26, 18:00:27] [0] \t\t #> Saving chunk 6: \t 21,332 passages and 4,102,886 embeddings. From #150,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7it [06:40, 57.28s/it]\n",
      "  0%|          | 0/7 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Oct 26, 18:00:30] [0] \t\t #> Checking all files were saved...\n",
      "[Oct 26, 18:00:30] [0] \t\t Found all files!\n",
      "[Oct 26, 18:00:30] [0] \t\t #> Building IVF...\n",
      "[Oct 26, 18:00:30] [0] \t\t #> Loading codes...\n",
      "[Oct 26, 18:00:30] [0] \t\t Sorting codes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:00<00:00, 154.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Oct 26, 18:00:32] [0] \t\t Getting unique codes...\n",
      "[Oct 26, 18:00:32] #> Optimizing IVF to store map from centroids to list of pids..\n",
      "[Oct 26, 18:00:32] #> Building the emb2pid mapping..\n",
      "[Oct 26, 18:00:33] len(emb2pid) = 29912015\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 65536/65536 [00:03<00:00, 20425.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Oct 26, 18:00:36] #> Saved optimized IVF to /home/bengsoon/Projects/xcs224u_project/zeroqaret/nbs/experiments/notebook/indexes/baseline_trec-covid.2bits/ivf.pid.pt\n",
      "[Oct 26, 18:00:36] [0] \t\t #> Saving the indexing metadata to /home/bengsoon/Projects/xcs224u_project/zeroqaret/nbs/experiments/notebook/indexes/baseline_trec-covid.2bits/metadata.json ..\n",
      "#> Joined...\n",
      "[Oct 26, 18:00:43] #> Loading codec...\n",
      "[Oct 26, 18:00:43] Loading decompress_residuals_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\n",
      "[Oct 26, 18:00:43] Loading packbits_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\n",
      "[Oct 26, 18:00:44] #> Loading IVF...\n",
      "[Oct 26, 18:00:44] #> Loading doclens...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████| 7/7 [00:00<00:00, 707.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Oct 26, 18:00:44] #> Loading codes and residuals...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|███████████████| 7/7 [00:00<00:00, 11.13it/s]\n"
     ]
    }
   ],
   "source": [
    "colbert_model = ColBERTRetrievalSearch(\n",
    "                                checkpoint, \n",
    "                                index_name, \n",
    "                                experiment_name, \n",
    "                                trec_colbert_collection, \n",
    "                                trec_collection_ids, \n",
    "                                doc_maxlen, \n",
    "                                nbits, \n",
    "                                kmeans, \n",
    "                                overwrite_param=True\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "colbert_retriever = EvaluateRetrieval(colbert_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#> QueryTokenizer.tensorize(batch_text[0], batch_background[0], bsize) ==\n",
      "#> Input: . what is the origin of COVID-19, \t\t True, \t\t None\n",
      "#> Output IDs: torch.Size([32]), tensor([  101,     1,  2054,  2003,  1996,  4761,  1997,  2522, 17258,  1011,\n",
      "         2539,   102,   103,   103,   103,   103,   103,   103,   103,   103,\n",
      "          103,   103,   103,   103,   103,   103,   103,   103,   103,   103,\n",
      "          103,   103])\n",
      "#> Output Mask: torch.Size([32]), tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "50it [00:01, 48.26it/s]\n"
     ]
    }
   ],
   "source": [
    "trec_colbert_results = colbert_retriever.retrieve(trec_colbert_collection, trec_colbert_queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "trec_results_collector.evaluate(\"ColBERT Baseline\", colbert_retriever, trec_qrels, trec_colbert_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SBERT Baseline</th>\n",
       "      <th>ColBERT Baseline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>NDCG@1</th>\n",
       "      <td>0.60000</td>\n",
       "      <td>0.79000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NDCG@3</th>\n",
       "      <td>0.54989</td>\n",
       "      <td>0.76061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NDCG@5</th>\n",
       "      <td>0.52905</td>\n",
       "      <td>0.76205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NDCG@10</th>\n",
       "      <td>0.51318</td>\n",
       "      <td>0.73627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NDCG@100</th>\n",
       "      <td>0.41738</td>\n",
       "      <td>0.53686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NDCG@1000</th>\n",
       "      <td>0.43220</td>\n",
       "      <td>0.45803</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           SBERT Baseline  ColBERT Baseline\n",
       "NDCG@1            0.60000           0.79000\n",
       "NDCG@3            0.54989           0.76061\n",
       "NDCG@5            0.52905           0.76205\n",
       "NDCG@10           0.51318           0.73627\n",
       "NDCG@100          0.41738           0.53686\n",
       "NDCG@1000         0.43220           0.45803"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trec_results_collector.ndcg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SBERT Baseline</th>\n",
       "      <th>ColBERT Baseline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>MAP@1</th>\n",
       "      <td>0.00195</td>\n",
       "      <td>0.00205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MAP@3</th>\n",
       "      <td>0.00465</td>\n",
       "      <td>0.00578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MAP@5</th>\n",
       "      <td>0.00672</td>\n",
       "      <td>0.00945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MAP@10</th>\n",
       "      <td>0.01218</td>\n",
       "      <td>0.01740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MAP@100</th>\n",
       "      <td>0.07061</td>\n",
       "      <td>0.09543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MAP@1000</th>\n",
       "      <td>0.19734</td>\n",
       "      <td>0.22305</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          SBERT Baseline  ColBERT Baseline\n",
       "MAP@1            0.00195           0.00205\n",
       "MAP@3            0.00465           0.00578\n",
       "MAP@5            0.00672           0.00945\n",
       "MAP@10           0.01218           0.01740\n",
       "MAP@100          0.07061           0.09543\n",
       "MAP@1000         0.19734           0.22305"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trec_results_collector.map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xcs224u",
   "language": "python",
   "name": "xcs224u"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
