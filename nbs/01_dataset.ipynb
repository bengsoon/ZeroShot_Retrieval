{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "\n",
    "> Dataset Preparation & Exploration: This is where we will prepare and explore the datasets for our experiment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from loguru import logger\n",
    "import os\n",
    "from pathlib import Path\n",
    "from fastcore.basics import patch_to, patch\n",
    "from fastcore.utils import  *\n",
    "\n",
    "from beir.datasets.data_loader import GenericDataLoader\n",
    "from beir import util\n",
    "\n",
    "from zeroqaret.helper import write_file\n",
    "\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "\n",
    "from typing import Union, List, Tuple\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "import ujson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BEIR datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "udapdr_list = ['arguana', 'webis-touche2020', 'trec-covid', 'nfcorpus', 'hotpotqa', 'dbpedia-entity', 'climate-fever', 'fever', 'scifact', 'scidocs',  'fiqa']\n",
    "\n",
    "#| export\n",
    "our_list = [\"fiqa\", \"trec-covid\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class BEIRDataset:\n",
    "    def __init__(self, \n",
    "                 url: str = \"https://public.ukp.informatik.tu-darmstadt.de/thakur/BEIR/datasets/{}.zip\"\n",
    "                ):\n",
    "        \"\"\" Wrapper to load and manage BEIR datasets.\"\"\"\n",
    "        # # split the names of the datasets if in string\n",
    "        # if datasets:\n",
    "        #     if isinstance(datasets, str):\n",
    "        #         self.datasets = datasets.split(\",\")\n",
    "        #     else:\n",
    "        #         self.datasets = datasets\n",
    "        # else: # if None\n",
    "        #     # these are all the public datasets available on https://public.ukp.informatik.tu-darmstadt.de/thakur/BEIR/datasets/ (retrieved 20231022)\n",
    "        #     self.datasets = ['fiqa', 'trec-covid']    \n",
    "\n",
    "        self.url = url\n",
    "        \n",
    "        ###  self.out_dir is where we will save all the datasets ###\n",
    "        try: \n",
    "            parent_dir = Path(__file__).absolute().parents[1]\n",
    "        except:\n",
    "            parent_dir = Path().absolute().parents[0]\n",
    "        self.out_dir = (parent_dir /  \"datasets\").as_posix()\n",
    "        logger.info(f\"Datasets will be saved in '{self.out_dir}'\")\n",
    "        \n",
    "        ### this is where we will store all the datasets with their corresponding paths ###\n",
    "        self.datasets = {}\n",
    "        \n",
    "    def load_dataset(self,\n",
    "                     dataset: str,\n",
    "                     split: str = \"test\",\n",
    "                    ): \n",
    "        \n",
    "        if dataset not in self.datasets:\n",
    "            logger.info(f\"Downloading dataset '{dataset}'...\")\n",
    "            cur_url = self.url.format(dataset)\n",
    "            dataset_path= util.download_and_unzip(cur_url, self.out_dir)\n",
    "            logger.info(f\"Saved on '{dataset_path}'\")\n",
    "    \n",
    "            self.datasets[dataset] = dataset_path\n",
    "\n",
    "        return GenericDataLoader(self.datasets[dataset]).load(split=split) # corpus, queries, qrels\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-10-26 09:30:01.422\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m25\u001b[0m - \u001b[1mDatasets will be saved in '/home/bengsoon/Projects/xcs224u_project/zeroqaret/datasets'\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "beir_dataset = BEIRDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-10-24 10:43:37.347\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mload_dataset\u001b[0m:\u001b[36m35\u001b[0m - \u001b[1mDownloading dataset 'trec-covid'...\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e8b48a288f1419bb2e353731b49d526",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "/home/bengsoon/Projects/xcs224u_project/zeroqaret/datasets/trec-covid.zip:   0%|          | 0.00/70.5M [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-10-24 10:45:07.605\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mload_dataset\u001b[0m:\u001b[36m38\u001b[0m - \u001b[1mSaved on '/home/bengsoon/Projects/xcs224u_project/zeroqaret/datasets/trec-covid'\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c06f62de810401cb01bc3b637de3168",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/171332 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trec_covid = beir_dataset.load_dataset(\"trec-covid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'trec-covid': '/home/bengsoon/Projects/xcs224u_project/zeroshot_qa_retrieval/datasets/trec-covid'}"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beir_dataset.datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch_to(BEIRDataset)\n",
    "def convert_for_colbert(self,\n",
    "                         dataset: str,\n",
    "                         split: str = \"test\",\n",
    "                        ) -> Tuple[str, str]: \n",
    "\n",
    "    \"\"\" \n",
    "    Converts downloaded BeIR datasets into tsv for ColBERT.\n",
    "    Returns (collection_path, queries_path)\n",
    "    \"\"\"\n",
    "    \n",
    "    # load corpus, queries, qrels\n",
    "    corpus, queries, qrels = self.load_dataset(dataset, split)\n",
    "    corpus_ids = list(corpus)\n",
    "    query_ids = list(queries)\n",
    "\n",
    "        \n",
    "    # make dir\n",
    "    os.makedirs(Path(self.datasets[dataset]) / \"colbert\", exist_ok=True)\n",
    "\n",
    "    # collection_path and queries_path\n",
    "    collection_path = (Path(self.datasets[dataset]) / \"colbert\" / f\"{dataset}_collection.tsv\").as_posix()\n",
    "    queries_path = (Path(self.datasets[dataset]) / \"colbert\" / f\"{dataset}_queries.tsv\").as_posix()\n",
    "\n",
    "    \n",
    "    logger.info(\"Preprocessing Corpus and Saving to {} ...\".format(collection_path))\n",
    "    with open(collection_path, 'w') as f_in:\n",
    "        writer = csv.writer(f_in, delimiter=\"\\t\", quoting=csv.QUOTE_MINIMAL)\n",
    "        for idx, doc_id in enumerate(tqdm(corpus_ids, total=len(corpus_ids))):\n",
    "            doc = corpus[doc_id]\n",
    "            writer.writerow([idx, (self.preprocess(doc.get(\"title\", \"\")) + \" \" + self.preprocess(doc.get(\"text\", \"\"))).strip(), doc_id])\n",
    "\n",
    "    logger.info(\"Preprocessing Corpus and Saving to {} ...\".format(queries_path))\n",
    "    with open(queries_path, 'w') as f_in:\n",
    "        writer = csv.writer(f_in, delimiter=\"\\t\", quoting=csv.QUOTE_MINIMAL)\n",
    "        for idx, query_id in enumerate(tqdm(query_ids, total=len(queries))):\n",
    "            query = queries[query_id]\n",
    "            writer.writerow([idx, query])\n",
    "\n",
    "    return collection_path, queries_path\n",
    "\n",
    "@patch_to(BEIRDataset)\n",
    "def preprocess(self, text):\n",
    "    return text.replace(\"\\r\", \" \").replace(\"\\t\", \" \").replace(\"\\n\", \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning!\n"
     ]
    }
   ],
   "source": [
    "overwrite = False\n",
    "mode = \"a\"\n",
    "\n",
    "if (overwrite) and (mode == \"a\") or (not overwrite):\n",
    "    print(\"Warning!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch_to(BEIRDataset)\n",
    "def prepare_qg_for_colbert_training(self,\n",
    "                                    csv_path: str, # path to CSV training file (containing pid, passage, question, title columns)\n",
    "                                    mode: str = \"a\", # mode to write the file (append by default)\n",
    "                                    replace: bool = False, # if True, will throw error if existing files are found and `mode = \"a\"`\n",
    "                                ) -> (str, str, str):\n",
    "    \"\"\"\n",
    "    Converting and preparing the dataframe loaded from `csv_path` (containing at least 'pid', 'passage' and 'question' columns) for training.\n",
    "    This will create the following:\n",
    "        - collection.tsv: TSV containing \"pid \\t passage text\"\n",
    "        - queries.tsv: TSV containing \"generated qid \\t title - query\"\n",
    "        - triples.jsonl: jsonl contianing [qid, pid+, pid-] list per line\n",
    "\n",
    "    Returns tuple of:\n",
    "        - Path to triples.jsonl\n",
    "        - Path to queries.tsv\n",
    "        - Path to collection.tsv\n",
    "    \"\"\"\n",
    "    \n",
    "    # this is the path to save the files\n",
    "    save_path = Path(csv_path).parent / \"colbert_training\"\n",
    "    # make directory if it does not exist\n",
    "    save_path.mkdir(exist_ok=True)\n",
    "    \n",
    "    if (replace) and (mode == \"a\") or (not replace):\n",
    "        assert not os.listdir(save_path), f\"'{save_path}' is not empty! Please ensure that the folder is backed up and cleared before proceeding!\"\n",
    "    \n",
    "    logger.info(f\"Creating ColBERT training files from {save_path}...\")\n",
    "    \n",
    "    # read csv as dataframe\n",
    "    train_df = pd.read_csv(csv_path)\n",
    "\n",
    "    # generate qid from {index}\n",
    "    train_df[\"qid\"] = train_df.index\n",
    "\n",
    "    # also use {index} as colbert_idx\n",
    "    train_df[\"colbert_idx\"] = train_df.index\n",
    "\n",
    "    # query as a combination of {title} - {question} \n",
    "    train_df[\"query\"] = train_df[\"title\"].astype(str) + \" - \" + train_df[\"question\"].astype(str)\n",
    "\n",
    "    # create a shuffled pids for negative sampling\n",
    "    shuffled_idx = train_df.sample(len(train_df))[\"colbert_idx\"]\n",
    "\n",
    "    # create collection.tsv, queries.tsv and triples.jsonl from train_df\n",
    "    for idx, idx_n, pid, passage, qid, query in tqdm(zip(train_df[\"colbert_idx\"], shuffled_idx, train_df[\"pid\"], train_df[\"passage\"], train_df[\"qid\"], train_df[\"query\"]), desc=\"Training files: \"):\n",
    "        write_file(f\"{save_path}/triples.jsonl\", ujson.dumps([qid, idx, idx_n])  + '\\n', mode=mode)\n",
    "        write_file(f\"{save_path}/queries.tsv\", f\"{qid} \\t {query} \\n\", mode=mode)\n",
    "        write_file(f\"{save_path}/collection.tsv\", f\"{idx} \\t {passage} \\t {pid} \\n\", mode=mode)\n",
    "\n",
    "    logger.info(f\"triples.jsonl, queries,tsv and collection.tsv files created in {save_path}.\")\n",
    "\n",
    "    return (f\"{save_path}/triples.jsonl\", f\"{save_path}/queries.tsv\", f\"{save_path}/collection.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-11-01 00:36:21.482\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m25\u001b[0m - \u001b[1mDatasets will be saved in '/home/bengsoon/Projects/xcs224u_project/zeroqaret/datasets'\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "beir_dataset = BEIRDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-11-01 00:36:29.534\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprepare_qg_for_colbert_training\u001b[0m:\u001b[36m21\u001b[0m - \u001b[1mCreating ColBERT training files from ../datasets/scifact/qg/colbert_training...\u001b[0m\n",
      "Training files: : 5183it [00:00, 22680.41it/s]\n",
      "\u001b[32m2023-11-01 00:36:29.946\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprepare_qg_for_colbert_training\u001b[0m:\u001b[36m44\u001b[0m - \u001b[1mtriples.jsonl, queries,tsv and collection.tsv files created in ../datasets/scifact/qg/colbert_training.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "beir_dataset.prepare_qg_for_colbert_training(\"../datasets/scifact/qg/scifact_qg_all.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-10-23 18:00:08.237\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mload_dataset\u001b[0m:\u001b[36m35\u001b[0m - \u001b[1mDownloading dataset 'trec-covid'...\u001b[0m\n",
      "\u001b[32m2023-10-23 18:00:08.239\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mload_dataset\u001b[0m:\u001b[36m38\u001b[0m - \u001b[1mSaved on '/home/bengsoon/Projects/xcs224u_project/zeroshot_qa_retrieval/datasets/trec-covid'\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92576b62d26742128c2d592539c11a10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/171332 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-10-23 18:00:09.665\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mconvert_for_colbert\u001b[0m:\u001b[36m23\u001b[0m - \u001b[1mPreprocessing Corpus and Saving to /home/bengsoon/Projects/xcs224u_project/zeroshot_qa_retrieval/datasets/trec-covid/colbert/trec-covid_collection.tsv ...\u001b[0m\n",
      "100%|██| 171332/171332 [00:03<00:00, 47369.46it/s]\n",
      "\u001b[32m2023-10-23 18:00:13.286\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mconvert_for_colbert\u001b[0m:\u001b[36m30\u001b[0m - \u001b[1mPreprocessing Corpus and Saving to /home/bengsoon/Projects/xcs224u_project/zeroshot_qa_retrieval/datasets/trec-covid/colbert/trec-covid_queries.tsv ...\u001b[0m\n",
      "100%|█████████| 50/50 [00:00<00:00, 365357.49it/s]\n"
     ]
    }
   ],
   "source": [
    "beir_dataset.convert_for_colbert(\"trec-covid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xcs224u",
   "language": "python",
   "name": "xcs224u"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
