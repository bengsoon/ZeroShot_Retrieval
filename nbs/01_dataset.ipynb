{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "\n",
    "> Dataset Preparation & Exploration: This is where we will prepare and explore the datasets for our experiment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from loguru import logger\n",
    "import os\n",
    "from pathlib import Path\n",
    "from fastcore.basics import patch_to, patch\n",
    "\n",
    "from beir.datasets.data_loader import GenericDataLoader\n",
    "from beir import util\n",
    "\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "\n",
    "from typing import Union, List, Tuple\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BEIR datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "udapdr_list = ['arguana', 'webis-touche2020', 'trec-covid', 'nfcorpus', 'hotpotqa', 'dbpedia-entity', 'climate-fever', 'fever', 'scifact', 'scidocs',  'fiqa']\n",
    "\n",
    "#| export\n",
    "our_list = [\"fiqa\", \"trec-covid\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class BEIRDataset:\n",
    "    def __init__(self, \n",
    "                 url: str = \"https://public.ukp.informatik.tu-darmstadt.de/thakur/BEIR/datasets/{}.zip\"\n",
    "                ):\n",
    "        \"\"\" Wrapper to load and manage BEIR datasets.\"\"\"\n",
    "        # # split the names of the datasets if in string\n",
    "        # if datasets:\n",
    "        #     if isinstance(datasets, str):\n",
    "        #         self.datasets = datasets.split(\",\")\n",
    "        #     else:\n",
    "        #         self.datasets = datasets\n",
    "        # else: # if None\n",
    "        #     # these are all the public datasets available on https://public.ukp.informatik.tu-darmstadt.de/thakur/BEIR/datasets/ (retrieved 20231022)\n",
    "        #     self.datasets = ['fiqa', 'trec-covid']    \n",
    "\n",
    "        self.url = url\n",
    "        \n",
    "        ###  self.out_dir is where we will save all the datasets ###\n",
    "        try: \n",
    "            parent_dir = Path(__file__).absolute().parents[1]\n",
    "        except:\n",
    "            parent_dir = Path().absolute().parents[0]\n",
    "        self.out_dir = (parent_dir /  \"datasets\").as_posix()\n",
    "        logger.info(f\"Datasets will be saved in '{self.out_dir}'\")\n",
    "        \n",
    "        ### this is where we will store all the datasets with their corresponding paths ###\n",
    "        self.datasets = {}\n",
    "        \n",
    "    def load_dataset(self,\n",
    "                     dataset: str,\n",
    "                     split: str = \"test\",\n",
    "                    ): \n",
    "        \n",
    "        if dataset not in self.datasets:\n",
    "            logger.info(f\"Downloading dataset '{dataset}'...\")\n",
    "            cur_url = self.url.format(dataset)\n",
    "            dataset_path= util.download_and_unzip(cur_url, self.out_dir)\n",
    "            logger.info(f\"Saved on '{dataset_path}'\")\n",
    "    \n",
    "            self.datasets[dataset] = dataset_path\n",
    "\n",
    "        return GenericDataLoader(self.datasets[dataset]).load(split=split) # corpus, queries, qrels\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-10-26 09:30:01.422\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m25\u001b[0m - \u001b[1mDatasets will be saved in '/home/bengsoon/Projects/xcs224u_project/zeroqaret/datasets'\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "beir_dataset = BEIRDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-10-24 10:43:37.347\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mload_dataset\u001b[0m:\u001b[36m35\u001b[0m - \u001b[1mDownloading dataset 'trec-covid'...\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e8b48a288f1419bb2e353731b49d526",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "/home/bengsoon/Projects/xcs224u_project/zeroqaret/datasets/trec-covid.zip:   0%|          | 0.00/70.5M [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-10-24 10:45:07.605\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mload_dataset\u001b[0m:\u001b[36m38\u001b[0m - \u001b[1mSaved on '/home/bengsoon/Projects/xcs224u_project/zeroqaret/datasets/trec-covid'\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c06f62de810401cb01bc3b637de3168",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/171332 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trec_covid = beir_dataset.load_dataset(\"trec-covid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'trec-covid': '/home/bengsoon/Projects/xcs224u_project/zeroshot_qa_retrieval/datasets/trec-covid'}"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beir_dataset.datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch_to(BEIRDataset)\n",
    "def convert_for_colbert(self,\n",
    "                         dataset: str,\n",
    "                         split: str = \"test\",\n",
    "                        ) -> Tuple[str, str]: \n",
    "\n",
    "    \"\"\" \n",
    "    Converts downloaded BeIR datasets into tsv for ColBERT.\n",
    "    Returns (collection_path, queries_path)\n",
    "    \"\"\"\n",
    "    \n",
    "    # load corpus, queries, qrels\n",
    "    corpus, queries, qrels = self.load_dataset(dataset, split)\n",
    "    corpus_ids = list(corpus)\n",
    "\n",
    "        \n",
    "    # make dir\n",
    "    os.makedirs(Path(self.datasets[dataset]) / \"colbert\", exist_ok=True)\n",
    "\n",
    "    # collection_path and queries_path\n",
    "    collection_path = (Path(self.datasets[dataset]) / \"colbert\" / f\"{dataset}_collection.tsv\").as_posix()\n",
    "    queries_path = (Path(self.datasets[dataset]) / \"colbert\" / f\"{dataset}_queries.tsv\").as_posix()\n",
    "\n",
    "    \n",
    "    logger.info(\"Preprocessing Corpus and Saving to {} ...\".format(collection_path))\n",
    "    with open(collection_path, 'w') as f_in:\n",
    "        writer = csv.writer(f_in, delimiter=\"\\t\", quoting=csv.QUOTE_MINIMAL)\n",
    "        for idx, doc_id in enumerate(tqdm(corpus_ids, total=len(corpus_ids))):\n",
    "            doc = corpus[doc_id]\n",
    "            writer.writerow([idx, (self.preprocess(doc.get(\"title\", \"\")) + \" \" + self.preprocess(doc.get(\"text\", \"\"))).strip(), doc_id])\n",
    "\n",
    "    logger.info(\"Preprocessing Corpus and Saving to {} ...\".format(queries_path))\n",
    "    with open(queries_path, 'w') as f_in:\n",
    "        writer = csv.writer(f_in, delimiter=\"\\t\", quoting=csv.QUOTE_MINIMAL)\n",
    "        for qid, query in tqdm(queries.items(), total=len(queries)):\n",
    "            writer.writerow([qid, query])\n",
    "\n",
    "    return collection_path, queries_path\n",
    "\n",
    "@patch_to(BEIRDataset)\n",
    "def preprocess(self, text):\n",
    "    return text.replace(\"\\r\", \" \").replace(\"\\t\", \" \").replace(\"\\n\", \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-10-26 09:30:07.455\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m25\u001b[0m - \u001b[1mDatasets will be saved in '/home/bengsoon/Projects/xcs224u_project/zeroqaret/datasets'\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "beir_dataset = BEIRDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bd87e2ce77a4e119af20d0611d242e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/57638 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-10-26 09:31:33.190\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mconvert_for_colbert\u001b[0m:\u001b[36m26\u001b[0m - \u001b[1mPreprocessing Corpus and Saving to /home/bengsoon/Projects/xcs224u_project/zeroqaret/datasets/fiqa/colbert/fiqa_collection.tsv ...\u001b[0m\n",
      "100%|████| 57638/57638 [00:00<00:00, 64175.72it/s]\n",
      "\u001b[32m2023-10-26 09:31:34.103\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mconvert_for_colbert\u001b[0m:\u001b[36m33\u001b[0m - \u001b[1mPreprocessing Corpus and Saving to /home/bengsoon/Projects/xcs224u_project/zeroqaret/datasets/fiqa/colbert/fiqa_queries.tsv ...\u001b[0m\n",
      "100%|███████| 648/648 [00:00<00:00, 398403.55it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('/home/bengsoon/Projects/xcs224u_project/zeroqaret/datasets/fiqa/colbert/fiqa_collection.tsv',\n",
       " '/home/bengsoon/Projects/xcs224u_project/zeroqaret/datasets/fiqa/colbert/fiqa_queries.tsv')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beir_dataset.convert_for_colbert(\"fiqa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-10-23 18:00:08.237\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mload_dataset\u001b[0m:\u001b[36m35\u001b[0m - \u001b[1mDownloading dataset 'trec-covid'...\u001b[0m\n",
      "\u001b[32m2023-10-23 18:00:08.239\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mload_dataset\u001b[0m:\u001b[36m38\u001b[0m - \u001b[1mSaved on '/home/bengsoon/Projects/xcs224u_project/zeroshot_qa_retrieval/datasets/trec-covid'\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92576b62d26742128c2d592539c11a10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/171332 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-10-23 18:00:09.665\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mconvert_for_colbert\u001b[0m:\u001b[36m23\u001b[0m - \u001b[1mPreprocessing Corpus and Saving to /home/bengsoon/Projects/xcs224u_project/zeroshot_qa_retrieval/datasets/trec-covid/colbert/trec-covid_collection.tsv ...\u001b[0m\n",
      "100%|██| 171332/171332 [00:03<00:00, 47369.46it/s]\n",
      "\u001b[32m2023-10-23 18:00:13.286\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mconvert_for_colbert\u001b[0m:\u001b[36m30\u001b[0m - \u001b[1mPreprocessing Corpus and Saving to /home/bengsoon/Projects/xcs224u_project/zeroshot_qa_retrieval/datasets/trec-covid/colbert/trec-covid_queries.tsv ...\u001b[0m\n",
      "100%|█████████| 50/50 [00:00<00:00, 365357.49it/s]\n"
     ]
    }
   ],
   "source": [
    "beir_dataset.convert_for_colbert(\"trec-covid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xcs224u",
   "language": "python",
   "name": "xcs224u"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
