# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/04_questions.ipynb.

# %% auto 0
__all__ = ['QuestionGenerator', 'generate_questions']

# %% ../nbs/04_questions.ipynb 5
from loguru import logger
import os
from pathlib import Path
from fastcore.basics import patch_to, patch
from typing import Union, List

from .dataset import BEIRDataset, our_list as eval_list
from .helper import get_today

import pandas as pd

import torch
from torch import Tensor

import textwrap

from colbert.modeling.colbert import ColBERT
from colbert.infra import Run, RunConfig, ColBERTConfig
from colbert.data import Queries, Collection

from tqdm import tqdm

# %% ../nbs/04_questions.ipynb 6
from langchain.llms import Ollama
from langchain.callbacks.manager import CallbackManager
from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler 
from langchain.schema import OutputParserException

from typing import Dict

# %% ../nbs/04_questions.ipynb 7
from langchain.prompts import PromptTemplate
from langchain.schema import StrOutputParser
from langchain.output_parsers import StructuredOutputParser, ResponseSchema

# %% ../nbs/04_questions.ipynb 8
from sentence_transformers import SentenceTransformer, util
import random

# %% ../nbs/04_questions.ipynb 55
class QuestionGenerator:

    def __init__(self, 
                 ollama_base_url: str = 'http://localhost:11434',
                 ollama_model_name: str = 'mistral:instruct',
                 random_seed: int = 158
                ):


        self.ollama_base_url = ollama_base_url
        self.ollama_model_name = ollama_model_name
        self.random_seed = random_seed
        
        # callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])
        
        self.llm = Ollama(
            base_url=ollama_base_url,
            model=ollama_model_name, 
                )
        
        self._setup_question_generator()
        self._setup_answer_checker()

    def _setup_question_generator(self,
                                 qg_prompt_template: str = None, # LLM Prompt template
                                ) -> None:
        " Sets up all the shared variables and methods for question generator "

        
        # Question Generation template
        self.qg_prompt_template = qg_prompt_template or """
<s>[INST]
You are a curious person who loves to ask pertinent questions. 
Given the Passage below, it is your job to give a correct highly descriptive title, ask the relevant right question and correct one-sentenced short answer strictly from the given passage.
----
{format_instructions}
---- 
Passage: {passage}
----
Title:
Question: 
Answer:
[/INST]"""

        # create response schemas
        self.qg_response_schemas = [
            # ResponseSchema(name="passage", description="Repeat of the input passage"), 
            ResponseSchema(name="title", description="Descriptive generated title based on the passage"),
            ResponseSchema(name="question", description="Relevant generated question based on the passage"),
            ResponseSchema(name="answer", description="Generated one-sentenced short answer based on the generated question and passage"),
        ]
        
        # create an output parser
        self.qg_output_parser = StructuredOutputParser.from_response_schemas(self.qg_response_schemas)
        
        # get format instructions to enforce the expected json format
        self.qg_format_instructions = self.qg_output_parser.get_format_instructions()

        # prompt template
        self.qg_prompt = PromptTemplate(
                                template=self.qg_prompt_template,
                                input_variables=["passage"],
                                partial_variables={"format_instructions": self.qg_format_instructions}
                            )

    def _setup_answer_checker(self,
                              ac_prompt_template: str = None, # LLM Prompt template
                             ) -> None:
        " Sets up all the shared variables and methods for the Answer Checker "
        
        self.answer_checker_prompt_template = """
<s>[INST]
You are an expert on the topic in the passage below. Given the Title, Passage and Question below, it is your job to provide a correct and relevant one-sentenced short answer.
----
{format_instructions}
----
Passage: {passage}
Title: {title}
Question: {question}
---- 
Answer:
[/INST]
"""

        # create response schemas
        self.answer_checker_response_schemas = [
            ResponseSchema(name="answer", description="Generated one-sentenced short answer based on the title, question and passage"),
        ]
        
        # create an output parser
        self.answer_checker_output_parser = StructuredOutputParser.from_response_schemas(self.answer_checker_response_schemas)
        
        # get format instructions to enforce the expected json format
        self.answer_checker_format_instructions = self.answer_checker_output_parser.get_format_instructions()

        # answer checker prompt
        self.answer_checker_prompt = PromptTemplate(
                template=self.answer_checker_prompt_template,
                input_variables=["passage", "title", "question"],
                partial_variables={"format_instructions": self.answer_checker_format_instructions}
            )

    def generate_question(self,
                          passage: str, # passage
                          random_seed: int = None, # if provided, it will replace random_seed
                          verbose: bool = False, # prints prompt
                          failure_count: int = None, # current count of failure to generate 
                          max_generation_tries: int = 10, # if failure_count == max_generation_tries, it will return the llm response params as empty strings 
                           **kwargs: "Any",
                         ) -> Dict[str, str]:
        """ 
        Prompts LLM to generate title, question and answer given `passage`. 
        Returns {"title": generated title, "question": generated question, "answer": generated answer }
        """ 

        random_seed=random_seed or self.random_seed
        
        prompt = self.qg_prompt.format_prompt(passage=passage).text
        if verbose in ["all"]: 
            logger.info("\n" + f"Question Generation Prompt:".center(100) + "\n\n" + textwrap.fill(prompt, 100))

        #####  The code block below checks to see if:
        #####      - the parser could parse the response from LLM
        #####      - the parsed results contains all the required params 
        # Inherit failure_count input, or start from 0 if it was passed
        if not failure_count:
            failure_count = 0
            
        success = False
        while success == False:
            logger.info(f"Current Question Generation failure_count: {failure_count}")
            try:
                res = self.llm(prompt, seed=random_seed, **kwargs)
                res = self.qg_output_parser.parse(res)
                assert self._output_is_dict(res), "Results were not parsed as dict!"
                assert self._check_params_in_output(res, ["title", "question", "answer"]), "Results does not contain all of ('title', 'question', 'answer')!"
                success = True
                
            except OutputParserException as e:
                logger.warning(f"Unable to parse results. Exception msg: {e}")
                failure_count += 1
                logger.info(f"Current failure count: {failure_count}")
                
            except AssertionError as e:
                logger.warning(e)
                failure_count += 1
                logger.info(f"Current failure count: {failure_count}")
                
            if failure_count >= max_generation_tries:
                logger.warning(f"#Error: Question Generation failure_count > max_generation_tries '{max_generation_tries}'." + "\{'title': '', 'question': '', 'answer': ''\}...")
                res = {"title": "", "question": "", "answer": ""}
                break

        for key, val in res.items():
            if type(val) == list:
                res[key] = " ".join(val)
                
        if verbose in ["all"]:
            logger.info("\n" + f"Question Generation Results:".center(100) + "\n\n" + textwrap.fill(str(res), 100))
        return res

    def generate_answer_to_check(self,
                                 passage: str, # passage
                                 title: str, # title
                                 question: str, # question
                                 random_seed: int = None, # if provided, it will replace random_seed
                                 verbose: bool = False, # prints prompt
                                 failure_count: int = None, # current count of failure to generate 
                                 max_generation_tries: int = 10, # if failure_count == max_generation_tries, it will return the llm response params as empty strings 
                                 **kwargs: "Any",
                                  ) -> Dict[str, str]:
        """ 
        Prompts LLM to answer given `passage`, `title`, `question`.
        Returns {"answer": generated answer }
        """ 

        random_seed=random_seed or self.random_seed
        
        prompt = self.answer_checker_prompt.format_prompt(passage=passage, title=title, question=question).text
        if verbose in ["all"]: 
            logger.info("\n" + f"Answer Checker Prompt:".center(100) + "\n\n" + textwrap.fill(prompt, 100))

        #####  The code block below checks to see if:
        #####      - the parser could parse the response from LLM
        #####      - the parsed results contains all the required params 
        # Inherit failure_count input, or start from 0 if it was passed
        if not failure_count:
            failure_count = 0
            
        success = False
        while success == False:
            logger.info(f"Current Question Generation failure_count: {failure_count}")
            try:
                res = self.llm(prompt, seed=random_seed, **kwargs)
                res = self.answer_checker_output_parser.parse(res)
                assert self._output_is_dict(res), "Results were not parsed as dict!"
                assert self._check_params_in_output(res, ["answer"]), "Results does not contain all of ('answer')!"
                success = True
                
            except OutputParserException as e:
                logger.warning(f"#Error: Unable to parse results. Exception msg: {e}")
                failure_count += 1
                logger.info(f"Current failure count: {failure_count}")
                
            except AssertionError as e:
                logger.warning(f"#Error: {e}")
                failure_count += 1
                logger.info(f"Current failure count: {failure_count}")
                
            if failure_count >= max_generation_tries:
                logger.warning(f"#Error: Answer Checker failure_count > max_generation_tries '{max_generation_tries}'." + "\{'answer': ''\}...")
                res = {"answer": ""}
                break

        for key, val in res.items():
            if type(val) == list:
                res[key] = " ".join(val)
        
        if verbose in ["all"]:
            logger.info("\n" + f"Answer Checker Results:".center(100) + "\n\n" + textwrap.fill(str(res), 100))
            
        return res

    def _output_is_dict(self,
                        res: Union[str, dict], # results from llm
                      ) -> bool:
        " Checks to see if output is dict. "

        return type(res) == dict

    def _check_params_in_output(self,
                                res: Union[str, dict], # results from llm
                                params: List, # list of keys (parameters) to test
                               ) -> bool:
        " Checks to see if res contains all the params as keys "
        
        return all([param in res for param in params])

    def round_trip_question_generation(self,
                                       passage: str, # passage
                                       embedding_model: str = 'all-MiniLM-L6-v2', # embedding model: any SBERT emb models or 'llm' if use emb from Ollama's llm
                                       cos_sim_cutoff: float = 0.8, # cosine-sim cutoff score to accept [-1, 1]
                                       random_seed: int = None, # if provided, it will replace random_seed
                                       max_generation_tries: int = 10, # if failure_count == max_generation_tries, it will return the llm response params as empty strings 
                                       verbose: str = None, # "all" to report everything, "results" to report only results. 
                                      ) -> Union[Dict[str, str], bool]:
        """ Prompts LLM to generate title, question and answer given `passage` and performs round-trip consistency check.
            Returns {"title": generated title, "question": generated question, "answer": generated answer} if passed, None if rejected.
            Note: We have assigned the generated_results as part of the instance variable of this class for debugging purposes when we loop through the corpus. """
        self.rt_random_seed = random_seed or self.random_seed
        
        if not hasattr(self, "emb_model"):
            logger.info("Setting up embedding model")
            if embedding_model == "llm":
                self.emb_model = OllamaEmbeddings(base_url=base_url, model=llm_model_name).embed_query
            else:
                self.emb_model = SentenceTransformer(embedding_model).encode
            
        logger.info("Generating question...")
        self.generated_results = self.generate_question(passage, verbose=verbose, random_seed=self.rt_random_seed, max_generation_tries=max_generation_tries)

        for key, val in self.generated_results.items():
            if type(val) == list:
                self.generated_results[key] = " ".join(val)
        
        logger.info("Performing round-trip consistency check...")
        self.checker_results = self.generate_answer_to_check(passage, self.generated_results["title"], self.generated_results["question"], verbose=verbose, random_seed=self.rt_random_seed, max_generation_tries=max_generation_tries)

        for key, val in self.checker_results.items():
            if type(val) == list:
                self.checker_results[key] = " ".join(val)
            
        if verbose in ["all", "results"]:
            logger.info("\n" + "." * 100 + "\n" + " Generated Answer: ".center(100, " ") + "\n\n" + textwrap.fill(f"{self.generated_results['answer']}", 100) + "\n\n" +
                        "\n" + " Checker's Answer: ".center(100, " ") + "\n\n" + textwrap.fill(f"{self.checker_results['answer']}", 100) + "\n\n" + "."*100)

        logger.info("Performing similarity calculation ...")
        gen_a_emb = self.emb_model(str(self.generated_results["answer"])) # force it to be string
        checker_a_emb = self.emb_model(str(self.checker_results["answer"])) # force it to be string

        score = self.cos_sim(gen_a_emb, checker_a_emb)
        if verbose in ["all", "results"]: 
            logger.info(f"Score: {score}")

        if score < cos_sim_cutoff:
            logger.info(f"Rejecting generated question set as it failed the consistency check")
            return None
        else:
            logger.info(f"Passed consistency check.")
            return self.generated_results        
            
        
    def cos_sim(self, a: Tensor, b: Tensor):
        """
        Computes the cosine similarity cos_sim(a[i], b[j]) for all i and j.
        
        :return: Matrix with res[i][j]  = cos_sim(a[i], b[j])
        Adapted from https://github.com/UKPLab/sentence-transformers/blob/c5f93f70eca933c78695c5bc686ceda59651ae3b/sentence_transformers/util.py
        """
        if not isinstance(a, torch.Tensor):
            a = torch.tensor(a)
    
        if not isinstance(b, torch.Tensor):
            b = torch.tensor(b)
    
        if len(a.shape) == 1:
            a = a.unsqueeze(0)
    
        if len(b.shape) == 1:
            b = b.unsqueeze(0)
    
        a_norm = torch.nn.functional.normalize(a, p=2, dim=1)
        b_norm = torch.nn.functional.normalize(b, p=2, dim=1)
        return torch.mm(a_norm, b_norm.transpose(0, 1)).to('cpu')

# %% ../nbs/04_questions.ipynb 57
@patch_to(QuestionGenerator)
def create_header(self,
                  msg: str, # message on the header
                 ) -> str:
    """ Creates a simple ASCII header """
    
    header = "   \n" + "".center(100, "*")
    header += "   \n" + "                                                                                        ".center(100, "*")
    header += "   \n" + f"                                    {msg}                                    ".center(100, "*")
    header += "   \n" + "                                                                                        ".center(100, "*")
    header += "   \n" + "".center(100, "*")
    
    return header

@patch_to(QuestionGenerator)
def generate_questions_corpus(self,
                              dataset_name: str, # name of the dataset
                              corpus: List,
                              corpus_ids: Dict,
                              df_checkpoint_path: str, # (csv format) checkpoint to save csv 
                              mode: str = "resume", # ["resume", "replace"] - resume if continue from checkpoint, replace if generation from scratch
                              cos_sim_cutoff: float = 0.8, # cosine-sim cutoff score to accept [-1, 1]
                              max_roundtrip_tries: int = 10, # maximum number of roundtrip question generation trials allowed for each document
                              max_generation_tries: int = 10, # maximum number of generation trials allowed independently for QG and Answer-Checker for each document
                              verbose: str = None, # "file" if log only to file w/o stdout (full verbosity), "all" if full verbosity, "results" if only results, "disable" will temporarily disable system logging.
                             ) -> None:
    assert verbose in [None, "all", "results", "disable", "file"], 'verbose options are only [None, "all", "results", "disable", "file"]'
    # disable logger if asked to
    if verbose == "disable": 
        logger.disable("__main__")
    else:
        logger.enable("__main__")

    # make checkpoint path directory if it does not exist:
    Path(df_checkpoint_path).parents[0].mkdir(exist_ok=True)
    
    if verbose == "file":
        logger.configure(handlers=[dict(sink=f"./{dataset_name}_qg_log.log", enqueue=True,format="[{time}] {message}", rotation=1e+7)]) # 100 MB
        verbose = "all" # set full verbosity logging to file
        
    if mode == "resume":
        # This is when we are resuming question generation from previous checkpoint.
        logger.info(f"Resuming question generation from checkpoint '{df_checkpoint_path}'")  
        try:
            qg_df = pd.read_csv(df_checkpoint_path, index_col=0) 
            qg_df["pid"] = qg_df["pid"].astype(str) # cast pid as str even though it may be just numbers
        except:
            raise Exception(f"Unable to read the checkpoint file '{df_checkpoint_path}', check the df_checkpoint_path or use `mode='replace'` to start from scratch.")
            
        last_checkpoint_idx = qg_df.iloc[-1].name # index corresponding to the key in corpus_ids
        last_checkpoint_pid = qg_df.iloc[-1].pid # pid corresponding to the val in corpus_ids

        # check that corpus_ids's correspond to the checkpoint index and pid
        assert corpus_ids[last_checkpoint_idx] == last_checkpoint_pid, "The corpus_ids \
provided does not tally with the idx: pid from dataframe's saved csv checkpoint. \
Please check corpus_ids input."
        starting_id = last_checkpoint_idx + 1 # we want to start one after our last one
        logger.info(f"Resuming from index {starting_id} / pid {corpus_ids[starting_id]}")
        
    elif mode == "replace":
        # we will start from scratch
        if Path(df_checkpoint_path).exists():
            raise Exception(f"{df_checkpoint_path} already exists! Please make a back up and remove the file before starting from scratch!")
        
        qg_df = pd.DataFrame(columns=["pid", "passage", "title", "question", "answer"])
        starting_id = 0   

    for idx in tqdm(range(starting_id, len(corpus)), "Question Generation Progress: "):
        pid = corpus_ids[idx]
        passage = corpus[idx]
        # logger.info("\n\n" + f"                     {idx} - {pid} ".center(150, "#") + "\n\n")
        logger.info(self.create_header(f"{idx} - {pid}"))
        logger.info("")
        
        similarity_cutoff = cos_sim_cutoff
        
        random_seed = self.random_seed
        success = False
        failure_count = 0
        while success == False:           
            # Performs roundtrip QG. Returns {"title": generated title, "question": generated question, "answer": generated answer} if passed, 
            #     returns None if rejected.
            res = self.round_trip_question_generation(passage, verbose=verbose, random_seed=random_seed, max_generation_tries=max_generation_tries, cos_sim_cutoff=similarity_cutoff) 

            ###############################   Code block below checks to see if ###############################
            #    - `res` is not None
            #    - `res` is dict 
            #    - `res` has the keys {"title": .., "question": .., "answer": ..}
            try:
                assert res, "Rejected by roundtrip check!"
                assert self._output_is_dict(res), "Results were not parsed as dict!"
                assert self._check_params_in_output(res, ["title", "question", "answer"]), "Results does not contain all of ('title', 'question', 'answer')!"
                success = True 
            except AssertionError as e:
                logger.warning(f"#Error: {e}")
                logger.info(f"Current roundtrip failure count: {failure_count}")
                logger.info("Regenerating another response...")
                failure_count += 1
            except:
                logger.warning("#Error: Error in roundtrip check")
                logger.info(f"Current roundtrip failure count: {failure_count}")
                logger.info("Regenerating another response...")
                failure_count += 1
            
            if max_roundtrip_tries - failure_count == 1:
                if similarity_cutoff > 0.2:
                    similarity_cutoff = similarity_cutoff - 0.1
                    logger.info(f"Setting cos-sim cutoff by 0.1...")
                random_seed = random.randint(200, 1000)
            elif failure_count >= max_roundtrip_tries:
                try:
                    assert self._output_is_dict(self.generated_results), "Results were not parsed as dict!"
                    assert self._check_params_in_output(self.generated_results, ["title", "question", "answer"]), "Results does not contain all of ('title', 'question', 'answer')!"
                    logger.warning(f"Maximum roundtrip tries exceeded '{max_roundtrip_tries}'." + f"Returning previously generated results: {self.generated_results}")
                    res = self.generated_results
                    break
                except:
                    logger.warning(f"Maximum roundtrip tries exceeded '{max_roundtrip_tries}'." + "Returning res = \{'title': '', 'question': '', 'answer': ''\}...")
                    res = {"title": "", "question": "", "answer": ""}
                    break
            else:
                random_seed = random.randint(200, 1000)
                
        res["passage"] = passage
        res["pid"] = pid
    
        qg_df = pd.concat((qg_df, pd.DataFrame(res, index=[idx])))
        logger.info(f"Saving dataframe checkpoint as '{df_checkpoint_path}.'")
        qg_df.to_csv(df_checkpoint_path)
    
    # enable logger back
    if verbose == "disable": logger.enable("__main__")

    logger.success("Question Generation is completed!")

# %% ../nbs/04_questions.ipynb 61
def generate_questions(dataset_name,
                       mode = "resume",
                       max_roundtrip_tries = 2,
                       max_generation_tries = 5,
                       cos_sim_cutoff = 0.6,
                       verbose = "file"):
    # initialize BeirDataset
    beir_dataset = BEIRDataset()
    
    # We want to load only the corpus / passages  
    raw_corpus, _, _ = beir_dataset.load_dataset(dataset_name)

    # get corpus_ids and corpus
    corpus_ids = {idx: str(val) for idx, val in enumerate(list(raw_corpus))}
    corpus = [(passage.get("title", "") + " " + passage["text"].strip()).strip() for passage in raw_corpus.values()] 

    # initialize QuestionGenerator()
    question_generator = QuestionGenerator()

    question_generator.generate_questions_corpus(dataset_name = dataset_name,
                                                 corpus = corpus, 
                                                 corpus_ids=corpus_ids, 
                                                 df_checkpoint_path=f"../datasets/{dataset_name}/qg/{dataset_name}_qg_all.csv", 
                                                 mode=mode, 
                                                 max_roundtrip_tries = max_roundtrip_tries,
                                                 cos_sim_cutoff = cos_sim_cutoff, 
                                                 max_generation_tries = max_generation_tries,
                                                 verbose=verbose)
                       
