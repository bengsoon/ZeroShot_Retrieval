# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/02_evaluation.ipynb.

# %% auto 0
__all__ = ['ResultsCollector', 'ColBERTRetrievalSearch']

# %% ../nbs/02_evaluation.ipynb 5
from loguru import logger
import os
from pathlib import Path
from fastcore.basics import patch_to, patch

from .helper import create_header
from .dataset import BEIRDataset, our_list as eval_list

from getpass import getpass
from typing import Union, Dict, List

from colbert import Indexer, Searcher
from colbert.infra import Run, RunConfig, ColBERTConfig
from colbert.data import Queries, Collection

import pandas as pd
from tqdm import tqdm

# %% ../nbs/02_evaluation.ipynb 12
class ResultsCollector:
    """ Collect results from Retrieval Evaluation for single dataset."""
    def __init__(self,
                 model_path: str = None,
                 dataset_name: str = None,
                 split: str = "test",
                 ):
        self.model_path = model_path
        self.dataset_name = dataset_name
        self.split = split
        logger.info("ResultsCollector object initialized.")
        
    def collect(self,
                experiment_name, 
                retriever,
                results,
                results_time):
        
        ndcg, map, recall, precision = retriever.evaluate(retriever.qrels, results, retriever.k_values)
        
        if not hasattr(self, "ndcg"): self.ndcg = pd.DataFrame()
        if not hasattr(self, "map"): self.map = pd.DataFrame()
        if not hasattr(self, "recall"): self.recall = pd.DataFrame()
        if not hasattr(self, "precision"): self.precision = pd.DataFrame()
        if not hasattr(self, "time"): self.time = pd.DataFrame()
        
        self.ndcg[experiment_name] = pd.Series(ndcg)
        self.map[experiment_name] = pd.Series(map)
        self.recall[experiment_name] = pd.Series(recall)
        self.precision[experiment_name] = pd.Series(precision)
        self.time[experiment_name] = pd.Series(results_time)

    @property
    def all(self):
        metrics = ["ndcg", "map", "recall", "precision", "time"]
        total_df = pd.DataFrame()
        for attr in self.__dir__():
            if attr in metrics:
                total_df = pd.concat((total_df, getattr(self, attr)))
        return total_df

            

# %% ../nbs/02_evaluation.ipynb 31
class ColBERTRetrievalSearch(Indexer):
    def __init__(self, 
                 checkpoint: str, # ColBERT checkpoint
                 index_name: str, # name of the index
                 experiment_name: str, # name of experiment
                 collection: "Collection", # collection object in Collection format
                 collection_ids: Dict, # {colbert_index: beir_pid}
                 doc_maxlen: int,
                 nbits: int,
                 kmeans: int = 4,
                 overwrite_param: Union[bool, str] = 'reuse',
                 **kwargs):
        """
        Retrieval Search wrapper for ColBERTv2, adapted from BeIR's `DenseRetrievalExactSearch`
         (https://github.com/beir-cellar/beir/blob/f062f038c4bfd19a8ca942a9910b1e0d218759d4/beir/retrieval/search/dense/exact_search.py#L12).

        The difference to BeIR's implementation is that if `corpus` and `corpus_ids` are passed at initialization stage, 
            it will pre-compute document encodings and store it. 

        If `index_name` and `overwrite = 'reuse'        
        """
        self.checkpoint = checkpoint
        self.index_name = index_name
        self.collection = collection
        self.collection_ids = collection_ids
        self.experiment_name = experiment_name
        self.doc_maxlen = doc_maxlen
        self.nbits = nbits
        self.kmeans = kmeans
        self.overwrite_param = overwrite_param
        
        with Run().context(RunConfig(nranks=1, experiment=experiment_name)):  # nranks specifies the number of GPUs to use
            config = ColBERTConfig(doc_maxlen=self.doc_maxlen, nbits=self.nbits, kmeans_niters=self.kmeans) # kmeans_niters specifies the number of iterations of k-means clustering; 4 is a good and fast default.
                                                                                        # Consider larger numbers for small datasets.
        
            super().__init__(checkpoint=self.checkpoint, config=config)
            self.index(name=self.index_name, collection=self.collection, overwrite=self.overwrite_param)
            
            self.searcher = Searcher(index=self.index_name, collection=self.collection)

    def search(self,
               corpus: "Collection" = None, # corpus in Collection format
               queries: "Queries" = None, # queries in Queries format
               k: int = 10, # top-K value
               score_function = None, # redundant; here to make it compatible with function call from EvaluateRetrieval
               filter_fn = None,              
               full_length_search: bool = False,
               **kwargs,
              ) -> Dict[str, Dict[str, float]]:

        res = self.searcher.search_all(queries, k, filter_fn, full_length_search)
        self.results = {}
        for qid, doc_res in res.items():
            doc_res = {self.collection_ids[cid] : score for cid, rank, score in doc_res}
            self.results[str(qid)] = doc_res

        return self.results
